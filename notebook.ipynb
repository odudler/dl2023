{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base libraries\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Callable, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# ML libraries\n",
    "import torch\n",
    "\n",
    "# Local imports\n",
    "from board import ConnectFourField\n",
    "from env import Env\n",
    "from agents.random_agent import RandomAgent\n",
    "from agents.deep_q_agent import DeepQAgent\n",
    "from agents.cql_agent import CQLAgent\n",
    "import utils\n",
    "\n",
    "# Fix random seed\n",
    "utils.seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train: Running episode 1 of 10.\n",
      " train: Running episode 2 of 10.\n",
      " train: Running episode 3 of 10.\n",
      " train: Running episode 4 of 10.\n",
      " train: Running episode 5 of 10.\n",
      " train: Running episode 6 of 10.\n",
      " train: Running episode 7 of 10.\n",
      " train: Running episode 8 of 10.\n",
      " train: Running episode 9 of 10.\n",
      " train: Running episode 10 of 10.\n",
      "Winner of episode 10 was player 2. P1 has 5 wins, P2 has 5 wins, and there were 0 draws.\n",
      "End state of the last game was:\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|2|0|0|0|0|0|\n",
      "|0|2|0|0|0|0|0|\n",
      "|1|2|0|0|0|0|0|\n",
      "|1|2|1|2|2|1|1|\n",
      "===============\n",
      " eval: Running episode 1 of 10.\n",
      " eval: Running episode 2 of 10.\n",
      " eval: Running episode 3 of 10.\n",
      " eval: Running episode 4 of 10.\n",
      " eval: Running episode 5 of 10.\n",
      " eval: Running episode 6 of 10.\n",
      " eval: Running episode 7 of 10.\n",
      " eval: Running episode 8 of 10.\n",
      " eval: Running episode 9 of 10.\n",
      " eval: Running episode 10 of 10.\n",
      "Results of evaluation were: P1 has 4 wins, P2 has 6 wins, and there were 0 draws.\n",
      "End state of the last game was:\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|2|0|0|0|0|\n",
      "|0|0|2|0|2|1|0|\n",
      "|0|0|1|0|2|1|0|\n",
      "|1|1|1|0|1|2|0|\n",
      "|1|2|1|2|2|2|2|\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "#Initialize Environment and Agent\n",
    "env = Env()\n",
    "agent = CQLAgent(env)\n",
    "\n",
    "AGENT = 1\n",
    "OPPONENT = 2\n",
    "NUM_EPISODES = {'train': 10, 'eval': 10}\n",
    "VERBOSE = False\n",
    "MODES = ['train', 'eval']\n",
    "UPDATE_OPPONENT = True\n",
    "OPPONENT_UPDATE_FREQUENCY = 50\n",
    "\n",
    "if VERBOSE: env.render_console()\n",
    "\n",
    "#Initialize Opponent Agent (This Agent is NOT trained)\n",
    "opponent = RandomAgent(env)\n",
    "\n",
    "for mode in MODES:\n",
    "    # Reset score counter\n",
    "    p1_score = 0\n",
    "    p2_score = 0\n",
    "\n",
    "    if mode == 'test' and type(opponent) is not RandomAgent:\n",
    "        # For evaluation, make opponent a random agent again\n",
    "        opponent = RandomAgent(env)\n",
    "\n",
    "    for i in range(1, NUM_EPISODES[mode] + 1):\n",
    "        print(f'\\r {mode}: Running episode {i} of {NUM_EPISODES[mode]}.', sep='')\n",
    "\n",
    "        # Initialize other variables\n",
    "        finished = -1\n",
    "\n",
    "        # Make it random who gets to start the game\n",
    "        # Set to true during the episode\n",
    "        agent_start = random.choice([True, False])\n",
    "        # Run one episode of the game\n",
    "        while finished == -1:\n",
    "            if finished != -1: break\n",
    "            # Agent makes a turn\n",
    "            if agent_start:\n",
    "                state = env.get_state()\n",
    "                action = agent.act(state)\n",
    "                if VERBOSE: print(f\"Agents Action: {action}\")\n",
    "                valid, reward, finished = agent.env.step(action, AGENT)\n",
    "                if VERBOSE: env.render_console()\n",
    "\n",
    "                # TODO: Here all the code for storing sequences in the buffer and learning/training the network would be!\n",
    "                if type(agent) is not RandomAgent and mode == 'train':\n",
    "                    agent.remember(state, action, reward, env.get_state(), finished)\n",
    "                    agent.optimize_model()\n",
    "                if finished != -1: break\n",
    "            else:\n",
    "                agent_start = True\n",
    "\n",
    "            # If move was invalid, repeat TODO: cumulate negative reward in this case!\n",
    "            '''\n",
    "            TODO: How to handle this whole \"invalid move\" situation in general,\n",
    "            1) Should we adapt the actionspace to only the valid actions? (hard..)\n",
    "            2) Punish the Agent for making a invalid move, but how to we represent that in the sequence?\n",
    "            -> Easiest way would probably just be to give negative reward and make the agent \"skip their move\",\n",
    "            i.e. they are not allowed to play a move (this punishes them aswell as they will more likely lose!)\n",
    "\n",
    "            Here I follow the approach that the Agent is NOT able to repeat the move if it was invalid!\n",
    "            TODO: In that case, the \"valid\" variable is unnecessary\n",
    "            '''\n",
    "\n",
    "            # Opponent makes their turn\n",
    "            action = opponent.act(env.get_state_inverted())\n",
    "            if VERBOSE: print(f\"Opponents Action: {action}\")\n",
    "            valid, reward, finished = opponent.env.step(action, OPPONENT)\n",
    "            if VERBOSE: env.render_console()\n",
    "            if finished != -1: break\n",
    "\n",
    "        # Update opponent with old versions of the agent, i.e. make the agent play against itself\n",
    "        if UPDATE_OPPONENT and mode == 'train' and i % OPPONENT_UPDATE_FREQUENCY == 0:\n",
    "            opponent = agent\n",
    "\n",
    "        episode_str = f'Winner of episode {i} was player {finished}.'\n",
    "        if finished == 1:\n",
    "            p1_score += 1\n",
    "        elif finished == 2:\n",
    "            p2_score += 1\n",
    "        else:\n",
    "            episode_str = f'Episode {i} ended in a draw.'\n",
    "\n",
    "        if VERBOSE or i == NUM_EPISODES[mode]:\n",
    "            if mode == 'eval': episode_str = 'Results of evaluation were:'\n",
    "            print(episode_str + f' P1 has {p1_score} wins, P2 has {p2_score} wins, and there were {i - p1_score - p2_score} draws.')\n",
    "            print('End state of the last game was:')\n",
    "            env.render_console()\n",
    "\n",
    "        env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
