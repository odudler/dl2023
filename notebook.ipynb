{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "# ML libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Local imports\n",
    "from env import Env\n",
    "from agents.random_agent import RandomAgent\n",
    "from agents.minimax_agent import MinimaxAgent\n",
    "from agents.minimax_agent_old import OldMinimaxAgent\n",
    "#from agents.deep_q_agent import DeepQAgent\n",
    "from agents.deep_q_agent_modified import DeepQAgent\n",
    "from agents.cql_agent import CQLAgent\n",
    "import utils\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# INITIALIZATION #\n",
    "##################\n",
    "\n",
    "# Fix random seed\n",
    "utils.seed_everything(42, deterministic=False)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define player and opponent IDs\n",
    "AGENT = 1\n",
    "OPPONENT = 2\n",
    "\n",
    "# Define game environment, this should be passed to the agents and the trainer\n",
    "env = Env()\n",
    "\n",
    "# Define agent and opponent\n",
    "agent = DeepQAgent(env=env, epsilon_max=1, epsilon_min=0.1, epsilon_decay=0.9999, device=device)\n",
    "opponent = MinimaxAgent(env=env, depth=3, epsilon=0.5, player=OPPONENT)\n",
    "replacement_agent = MinimaxAgent(env=env, depth=3, epsilon=0.5, player=OPPONENT)\n",
    "\n",
    "# Define options for training\n",
    "options = {\n",
    "           'UPDATE_OPPONENT': True,                         # Whether to enable self-play or not\n",
    "           'OPPONENT_UPDATE_FREQUENCY': 100,                # After how many episodes the opponent will be replaced by the current agent\n",
    "           'BOOTSTRAP_EPISODES': 7500,                      # During this time, the agent will not be replaced by itself\n",
    "           'DECAY_RANDOMNESS_OPPONENT': True,               # Decay randomness of the opponent. Use only if the opponent acts with some randomness\n",
    "           'DECAY_RANDOMNESS_FREQUENCY': 1000,              # Frequency of randomness decay\n",
    "           'REPLACE_FOR_EVALUATION': True,                  # Whether to replace the training model at the end with another evaluation model\n",
    "           'REPLACE_FOR_EVALUATION_BY': replacement_agent,  # Evalutation model to replace training model by\n",
    "           'AUTOSAVE': True,                                # Whether to save the model at certain intervals\n",
    "           'AUTOSAVE_TYPE': 'NUM_EPISODES',                 # One of [\"NUM_OPTIMIZATIONS\", \"NUM_EPISODES\"]\n",
    "           'AUTOSAVE_PERIOD': 1000,                         # After how many _ to save the model\n",
    "           }\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(env=env, agent=agent, opponent=opponent, agent_id=AGENT, opponent_id=OPPONENT, num_episodes={'TRAIN': 10000, 'EVAL': 100}, device=device, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "# Train agent\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: Running episode 1 of 1. Ratios are [WINS: 0.00% | LOSSES: 0.00% | TIES: 100.00%]\n",
      "\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|1|0|0|\n",
      "===============\n",
      "AGENT action was 4\n",
      "\n",
      "\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|2|0|1|0|0|\n",
      "===============\n",
      "OPPONENT action was 2\n",
      "\n",
      "\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|1|0|0|\n",
      "|0|0|2|0|1|0|0|\n",
      "===============\n",
      "AGENT action was 4\n",
      "\n",
      "\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|1|0|0|\n",
      "|0|0|2|2|1|0|0|\n",
      "===============\n",
      "OPPONENT action was 3\n",
      "\n",
      "\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|1|0|0|\n",
      "|0|0|0|0|1|0|0|\n",
      "|0|0|2|2|1|0|0|\n",
      "===============\n",
      "AGENT action was 4\n",
      "\n",
      "\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|1|0|0|\n",
      "|0|0|0|2|1|0|0|\n",
      "|0|0|2|2|1|0|0|\n",
      "===============\n",
      "OPPONENT action was 3\n",
      "\n",
      "\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|1|0|0|\n",
      "|0|0|0|0|1|0|0|\n",
      "|0|0|0|2|1|0|0|\n",
      "|0|0|2|2|1|0|0|\n",
      "===============\n",
      "AGENT action was 4\n",
      "\n",
      "EVAL: Average turns per episode 4.0\n",
      "EVAL: Average invalid moves per episode 0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# QUICK EVALUATION #\n",
    "####################\n",
    "\n",
    "# Fix random seed\n",
    "utils.seed_everything(42, deterministic=False)\n",
    "\n",
    "eval_env = Env()\n",
    "# Load agent from save and set to eval mode\n",
    "new_agent = CQLAgent(eval_env)\n",
    "new_agent.load_model('./saved_models/CQLAgent_58443.pt')\n",
    "new_agent.eval_mode()\n",
    "\n",
    "# Define opponent\n",
    "new_opponent = MinimaxAgent(eval_env, depth=6, epsilon=0.1, player=OPPONENT)\n",
    "\n",
    "# Run evaluation\n",
    "trainer.eval(new_agent, new_opponent, episodes=1, agent_start=None, print_last_n_games=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
