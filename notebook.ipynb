{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reload_ext dotenv\n",
    "#%dotenv\n",
    "\n",
    "# ML libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Local imports\n",
    "from env import Env\n",
    "from agents.random_agent import RandomAgent\n",
    "from agents.minimax_agent import MinimaxAgent\n",
    "from agents.minimax_agent_old import OldMinimaxAgent\n",
    "#from agents.deep_q_agent import DeepQAgent\n",
    "from agents.deep_q_agent_modified import DeepQAgent\n",
    "from agents.cql_agent import CQLAgent\n",
    "import utils\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/albin/miniconda3/envs/dl-proj/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# INITIALIZATION #\n",
    "##################\n",
    "\n",
    "# Fix random seed\n",
    "utils.seed_everything(42, deterministic=False)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define player and opponent IDs\n",
    "AGENT = 1\n",
    "OPPONENT = 2\n",
    "\n",
    "# Define game environment, this should be passed to the agents and the trainer\n",
    "env = Env()\n",
    "\n",
    "# Define agent and opponent\n",
    "agent = DeepQAgent(env=env, epsilon_max=1, epsilon_min=0.1, epsilon_decay=0.9999, device=device)\n",
    "opponent = MinimaxAgent(env=env, depth=3, epsilon=0.5, player=OPPONENT)\n",
    "replacement_agent = MinimaxAgent(env=env, depth=3, epsilon=0.5, player=OPPONENT)\n",
    "\n",
    "# Define options for training\n",
    "options = {\n",
    "           'UPDATE_OPPONENT': True,                         # Whether to enable self-play or not\n",
    "           'OPPONENT_UPDATE_FREQUENCY': 100,                # After how many episodes the opponent will be replaced by the current agent\n",
    "           'BOOTSTRAP_EPISODES': 7500,                      # During this time, the agent will not be replaced by itself\n",
    "           'DECAY_RANDOMNESS_OPPONENT': True,               # Decay randomness of the opponent. Use only if the opponent acts with some randomness\n",
    "           'DECAY_RANDOMNESS_FREQUENCY': 1000,              # Frequency of randomness decay\n",
    "           'REPLACE_FOR_EVALUATION': True,                  # Whether to replace the training model at the end with another evaluation model\n",
    "           'REPLACE_FOR_EVALUATION_BY': replacement_agent,  # Evalutation model to replace training model by\n",
    "           'AUTOSAVE': True,                                # Whether to save the model at certain intervals\n",
    "           'AUTOSAVE_TYPE': 'NUM_EPISODES',                 # One of [\"NUM_OPTIMIZATIONS\", \"NUM_EPISODES\"]\n",
    "           'AUTOSAVE_PERIOD': 1000,                         # After how many _ to save the model\n",
    "           }\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(env=env, agent=agent, opponent=opponent, agent_id=AGENT, opponent_id=OPPONENT, num_episodes={'TRAIN': 10000, 'EVAL': 100}, device=device, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "# Train agent\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: Running episode 1 of 2. Ratios are [WINS: 0.00% | LOSSES: 0.00% | TIES: 100.00%]This is inside Trainer: <env.Env object at 0x7f165a6dfcd0>\n",
      "This is inside minimax_agent: <env.Env object at 0x7f165a6dfcd0>\n",
      "\n",
      "\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|1|0|0|0|\n",
      "===============\n",
      "AGENT action was 3\n",
      "This is inside minimax_agent: <env.Env object at 0x7f165a6dfcd0>\n",
      "\n",
      "\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|2|0|0|0|\n",
      "|0|0|0|1|0|0|0|\n",
      "===============\n",
      "OPPONENT action was 3\n",
      "This is inside Trainer: <env.Env object at 0x7f165a6dfcd0>\n",
      "This is inside minimax_agent: <env.Env object at 0x7f165a6dfcd0>\n",
      "\n",
      "\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|2|0|0|0|\n",
      "|0|0|0|1|1|0|0|\n",
      "===============\n",
      "AGENT action was 4\n",
      "This is inside minimax_agent: <env.Env object at 0x7f165a6dfcd0>\n",
      "\n",
      "\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|2|0|0|0|\n",
      "|0|0|2|1|1|0|0|\n",
      "===============\n",
      "OPPONENT action was 2\n",
      "This is inside Trainer: <env.Env object at 0x7f165a6dfcd0>\n",
      "This is inside minimax_agent: <env.Env object at 0x7f165a6dfcd0>\n",
      "\n",
      "\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|2|0|0|0|\n",
      "|0|0|2|1|1|1|0|\n",
      "===============\n",
      "AGENT action was 5\n",
      "This is inside minimax_agent: <env.Env object at 0x7f165a6dfcd0>\n",
      "\n",
      "\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|2|0|0|0|\n",
      "|0|0|2|1|1|1|2|\n",
      "===============\n",
      "OPPONENT action was 6\n",
      "This is inside Trainer: <env.Env object at 0x7f165a6dfcd0>\n",
      "This is inside minimax_agent: <env.Env object at 0x7f165a6dfcd0>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(env\u001b[38;5;241m=\u001b[39meval_env, agent\u001b[38;5;241m=\u001b[39magent, opponent\u001b[38;5;241m=\u001b[39mopponent, agent_id\u001b[38;5;241m=\u001b[39mAGENT, opponent_id\u001b[38;5;241m=\u001b[39mOPPONENT, num_episodes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRAIN\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10000\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEVAL\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m}, device\u001b[38;5;241m=\u001b[39mdevice, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminmax_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopponent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminmax_opponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_last_n_games\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dl/dl-proj/dl2023/trainer.py:310\u001b[0m, in \u001b[0;36mTrainer.eval\u001b[0;34m(self, agent, opponent, episodes, agent_start, print_last_n_games)\u001b[0m\n\u001b[1;32m    308\u001b[0m start \u001b[38;5;241m=\u001b[39m agent_start \u001b[38;5;28;01mif\u001b[39;00m agent_start \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m random\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m])\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# Run one episode of the game and update running variables\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m finished, turns, invalid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEVAL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopponent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mturns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mturns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minvalid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minvalid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_game\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mprint_last_n_games\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# Update scores with winner\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m finished \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/dl/dl-proj/dl2023/trainer.py:189\u001b[0m, in \u001b[0;36mTrainer.play_episode\u001b[0;34m(self, mode, agent, opponent, agent_start, turns, invalid, print_game)\u001b[0m\n\u001b[1;32m    187\u001b[0m deterministic \u001b[38;5;241m=\u001b[39m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEVAL\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEST\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# Don't want randomness during evaluation\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Agent can use deterministic parameter if it has a use for it, otherwise it will be ignored\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Execute action\u001b[39;00m\n\u001b[1;32m    191\u001b[0m valid, reward, finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mAGENT)\n",
      "File \u001b[0;32m~/dl/dl-proj/dl2023/agents/minimax_agent.py:39\u001b[0m, in \u001b[0;36mMinimaxAgent.act\u001b[0;34m(self, state, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is inside minimax_agent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_predicted_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrandom_valid_action()\n",
      "File \u001b[0;32m~/dl/dl-proj/dl2023/agents/minimax_agent.py:90\u001b[0m, in \u001b[0;36mMinimaxAgent.best_predicted_action\u001b[0;34m(self, board, depth, player)\u001b[0m\n\u001b[1;32m     87\u001b[0m tempBoard\u001b[38;5;241m.\u001b[39mplay(player, move)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Call min on that new board\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m boardScore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimizeBeta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtempBoard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopponent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m boardScore \u001b[38;5;241m>\u001b[39m bestScore:\n\u001b[1;32m     92\u001b[0m     bestScore \u001b[38;5;241m=\u001b[39m boardScore\n",
      "File \u001b[0;32m~/dl/dl-proj/dl2023/agents/minimax_agent.py:115\u001b[0m, in \u001b[0;36mMinimaxAgent.minimizeBeta\u001b[0;34m(self, board, depth, a, b, player, opponent)\u001b[0m\n\u001b[1;32m    113\u001b[0m     tempBoard \u001b[38;5;241m=\u001b[39m deepcopy(board)\n\u001b[1;32m    114\u001b[0m     tempBoard\u001b[38;5;241m.\u001b[39mplay(opponent, move)\n\u001b[0;32m--> 115\u001b[0m     boardScore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximizeAlpha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtempBoard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopponent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m boardScore \u001b[38;5;241m<\u001b[39m beta:\n\u001b[1;32m    117\u001b[0m     beta \u001b[38;5;241m=\u001b[39m boardScore\n",
      "File \u001b[0;32m~/dl/dl-proj/dl2023/agents/minimax_agent.py:138\u001b[0m, in \u001b[0;36mMinimaxAgent.maximizeAlpha\u001b[0;34m(self, board, depth, a, b, player, opponent)\u001b[0m\n\u001b[1;32m    136\u001b[0m     tempBoard \u001b[38;5;241m=\u001b[39m deepcopy(board)\n\u001b[1;32m    137\u001b[0m     tempBoard\u001b[38;5;241m.\u001b[39mplay(player, move)\n\u001b[0;32m--> 138\u001b[0m     boardScore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimizeBeta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtempBoard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopponent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m boardScore \u001b[38;5;241m>\u001b[39m alpha:\n\u001b[1;32m    140\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m boardScore\n",
      "File \u001b[0;32m~/dl/dl-proj/dl2023/agents/minimax_agent.py:115\u001b[0m, in \u001b[0;36mMinimaxAgent.minimizeBeta\u001b[0;34m(self, board, depth, a, b, player, opponent)\u001b[0m\n\u001b[1;32m    113\u001b[0m     tempBoard \u001b[38;5;241m=\u001b[39m deepcopy(board)\n\u001b[1;32m    114\u001b[0m     tempBoard\u001b[38;5;241m.\u001b[39mplay(opponent, move)\n\u001b[0;32m--> 115\u001b[0m     boardScore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximizeAlpha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtempBoard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopponent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m boardScore \u001b[38;5;241m<\u001b[39m beta:\n\u001b[1;32m    117\u001b[0m     beta \u001b[38;5;241m=\u001b[39m boardScore\n",
      "File \u001b[0;32m~/dl/dl-proj/dl2023/agents/minimax_agent.py:138\u001b[0m, in \u001b[0;36mMinimaxAgent.maximizeAlpha\u001b[0;34m(self, board, depth, a, b, player, opponent)\u001b[0m\n\u001b[1;32m    136\u001b[0m     tempBoard \u001b[38;5;241m=\u001b[39m deepcopy(board)\n\u001b[1;32m    137\u001b[0m     tempBoard\u001b[38;5;241m.\u001b[39mplay(player, move)\n\u001b[0;32m--> 138\u001b[0m     boardScore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimizeBeta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtempBoard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopponent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m boardScore \u001b[38;5;241m>\u001b[39m alpha:\n\u001b[1;32m    140\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m boardScore\n",
      "File \u001b[0;32m~/dl/dl-proj/dl2023/agents/minimax_agent.py:104\u001b[0m, in \u001b[0;36mMinimaxAgent.minimizeBeta\u001b[0;34m(self, board, depth, a, b, player, opponent)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# RETURN CONDITION\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Check to see if game over\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(validMoves) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m board\u001b[38;5;241m.\u001b[39mis_finished() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mboard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutilityValue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# CONTINUE TREE SEARCH\u001b[39;00m\n\u001b[1;32m    107\u001b[0m beta \u001b[38;5;241m=\u001b[39m b\n",
      "File \u001b[0;32m~/dl/dl-proj/dl2023/board.py:255\u001b[0m, in \u001b[0;36mConnectFourField.utilityValue\u001b[0;34m(self, player)\u001b[0m\n\u001b[1;32m    252\u001b[0m playerScore    \u001b[38;5;241m=\u001b[39m p4s \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m99999\u001b[39m \u001b[38;5;241m+\u001b[39m p3s \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m999\u001b[39m \u001b[38;5;241m+\u001b[39m p2s \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m99\u001b[39m\n\u001b[1;32m    254\u001b[0m o4s  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcountSequence(opponent, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m--> 255\u001b[0m o3s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcountSequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m o2s   \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcountSequence(opponent, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    257\u001b[0m opponentScore  \u001b[38;5;241m=\u001b[39m o4s \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m99999\u001b[39m \u001b[38;5;241m+\u001b[39m o3s \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m999\u001b[39m \u001b[38;5;241m+\u001b[39m o2s \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m99\u001b[39m\n",
      "File \u001b[0;32m~/dl/dl-proj/dl2023/board.py:235\u001b[0m, in \u001b[0;36mConnectFourField.countSequence\u001b[0;34m(self, player, length)\u001b[0m\n\u001b[1;32m    233\u001b[0m             totalCount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m horizontalSeq(row, col)\n\u001b[1;32m    234\u001b[0m             \u001b[38;5;66;03m# check if a diagonal (both +ve and -ve slopes) four-in-a-row starts at (row, col)\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m             totalCount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[43mposDiagonalSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m negDiagonalSeq(row, col))\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# return the sum of sequences of length 'length'\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m totalCount\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####################\n",
    "# QUICK EVALUATION #\n",
    "####################\n",
    "\n",
    "# Fix random seed\n",
    "utils.seed_everything(42, deterministic=False)\n",
    "\n",
    "# Define player and opponent IDs\n",
    "AGENT = 1\n",
    "OPPONENT = 2\n",
    "\n",
    "# Fix random seed\n",
    "utils.seed_everything(42, deterministic=False)\n",
    "\n",
    "eval_env = Env()\n",
    "# Load agent from save and set to eval mode\n",
    "minmax_agent = MinimaxAgent(env=eval_env, depth=5, epsilon=0.0, player=AGENT)\n",
    "\n",
    "# Define opponent\n",
    "minmax_opponent = MinimaxAgent(env=eval_env, depth=5, epsilon=0.0, player=OPPONENT)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(env=eval_env, agent=agent, opponent=opponent, agent_id=AGENT, opponent_id=OPPONENT, num_episodes={'TRAIN': 10000, 'EVAL': 100}, device=device, verbose=True)\n",
    "# Run evaluation\n",
    "trainer.eval(agent=minmax_agent, opponent=minmax_opponent, episodes=2, agent_start=None, print_last_n_games=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
