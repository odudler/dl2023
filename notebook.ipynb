{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "# Base libraries\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Callable, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from typing import Union\n",
    "\n",
    "# ML libraries\n",
    "import torch\n",
    "\n",
    "# Local imports\n",
    "from board import ConnectFourField\n",
    "from env import Env\n",
    "from agents.random_agent import RandomAgent\n",
    "from agents.minimax_agent import MinimaxAgent\n",
    "from agents.minimax_agent_old import OldMinimaxAgent\n",
    "from agents.deep_q_agent import DeepQAgent\n",
    "from agents.cql_agent import CQLAgent\n",
    "import utils\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TRAIN: Running episode 999 of 1000. Current win ratio of AGENT is 8.81%.                                               TRAIN: Average turns per episode 10.164\n",
      "TRAIN: Average invalid moves per episode 0.15\n",
      "\n",
      "\n",
      " EVAL: Running episode 99 of 100. Current win ratio of AGENT is 9.09%..EVAL: Average turns per episode 10.0\n",
      "EVAL: Average invalid moves per episode 0.29\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fix random seed\n",
    "utils.seed_everything(42, deterministic=True)\n",
    "\n",
    "# Use GPU if available\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define player and opponent IDs\n",
    "AGENT = 1\n",
    "OPPONENT = 2\n",
    "\n",
    "# Define agent and opponent\n",
    "agent = DeepQAgent(device=device)\n",
    "opponent = MinimaxAgent(depth=3, player=OPPONENT)\n",
    "replacement_agent = RandomAgent()\n",
    "\n",
    "# Define options for training\n",
    "options = {'BOOTSTRAP_EPISODES': 200, \n",
    "           'UPDATE_OPPONENT': False, \n",
    "           'DECAY_RANDOMNESS_OPPONENT': True, \n",
    "           'DECAY_RANDOMNESS_FREQUENCY': 250,\n",
    "           'REPLACE_FOR_EVALUATION': True,\n",
    "           'REPLACE_FOR_EVALUATION_BY': replacement_agent,\n",
    "           }\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(env=Env(), agent=agent, opponent=opponent, agent_id=AGENT, opponent_id=OPPONENT, num_episodes={'TRAIN': 100, 'EVAL': 100}, device=device, verbose=True, options=options)\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AGENT = 1\n",
    "OPPONENT = 2\n",
    "NUM_EPISODES = {'TRAIN': 2500, 'EVAL': 100} # Define number of episodes for training and evaluation \n",
    "VERBOSE = False\n",
    "MODES = ['TRAIN', 'EVAL'] # Loop modes\n",
    "UPDATE_OPPONENT = False # Wether to update opponent with current player\n",
    "OPPONENT_UPDATE_FREQUENCY = 50 # Update opponent with current player every n episodes\n",
    "BOOTSTRAP_EPISODES = 500 # Episodes where opponent is just random agent\n",
    "DECAY_RANDOMNESS_OPPONENT = False\n",
    "DECAY_RANDOMNESS_FREQUENCY = 100\n",
    "\n",
    "if VERBOSE: env.render_console()\n",
    "\n",
    "# agent = DeepQAgent(env, epsilon_max = 1, epsilon_min = 0.01, device=device)\n",
    "agent = DeepQAgent(env=env, hidden_layers=2, batch_size=4, epsilon_max=0.8, device=device)\n",
    "# Initialize Opponent Agent (This Agent is NOT trained)\n",
    "opponent = MinimaxAgent(depth=3, epsilon=0.5, player=OPPONENT)\n",
    "\n",
    "for mode in MODES:\n",
    "    # Reset score counter\n",
    "    p1_score = 0\n",
    "    p2_score = 0\n",
    "\n",
    "    turns = 0\n",
    "    invalid = 0\n",
    "\n",
    "    # if mode == 'EVAL':# and type(opponent) is not RandomAgent:\n",
    "    #     print('Changing agent to random agent for evaluation.')\n",
    "    #     # For evaluation, make opponent a random agent again\n",
    "    #     opponent = MinimaxAlphaBeta(depth=3, epsilon=0.5, player=OPPONENT)\n",
    "\n",
    "    for i in range(1, NUM_EPISODES[mode] + 1):\n",
    "        # Clean up terminal line \n",
    "        if i % 100 == 0: print('\\r', '                                                                                                                       ', end='')\n",
    "        # Print current episode\n",
    "        print('\\r', f'{mode}: Running episode {i} of {NUM_EPISODES[mode]}. Current win ratio of AGENT is {p1_score / i:.2%}.', end='')\n",
    "\n",
    "        # Print the last game\n",
    "        if i == NUM_EPISODES[mode] and mode == 'EVAL':\n",
    "            VERBOSE = True\n",
    "\n",
    "        # Initialize other variables\n",
    "        finished = -1\n",
    "\n",
    "        # Make it random who gets to start the game\n",
    "        # Set to true during the episode\n",
    "        agent_start = random.choice([True, False])\n",
    "        # Run one episode of the game\n",
    "        while finished == -1:\n",
    "            # Episode is finished\n",
    "            if finished != -1: break\n",
    "            # Agent makes a turn\n",
    "            if agent_start:\n",
    "                state = env.get_state('class' if type(agent) in [MinimaxAgent] else 'list')\n",
    "                action = agent.act(env if type(agent) is MinimaxAgent else state)\n",
    "                if VERBOSE or i >= NUM_EPISODES[mode] - 2: print(f\"Agents Action: {action}\")\n",
    "                valid, reward, finished = env.step(action, AGENT)\n",
    "\n",
    "                # valid, reward, finished = agent.env.step(action, AGENT)\n",
    "                turns += 1\n",
    "                if valid == -1: invalid += 1\n",
    "                if VERBOSE or i == NUM_EPISODES[mode]: env.render_console()\n",
    "\n",
    "                # TODO: Here all the code for storing sequences in the buffer and learning/training the network would be!\n",
    "                if type(agent) not in [RandomAgent, MinimaxAgent] and mode == 'TRAIN':\n",
    "                    agent.remember(state, action, reward, env.get_state(return_type='class'), finished)\n",
    "                    agent.optimize_model()\n",
    "                if finished != -1: break\n",
    "            else:\n",
    "                agent_start = True\n",
    "\n",
    "            # If move was invalid, repeat TODO: cumulate negative reward in this case!\n",
    "            '''\n",
    "            TODO: How to handle this whole \"invalid move\" situation in general,\n",
    "            1) Should we adapt the actionspace to only the valid actions? (hard..)\n",
    "            2) Punish the Agent for making a invalid move, but how to we represent that in the sequence?\n",
    "            -> Easiest way would probably just be to give negative reward and make the agent \"skip their move\",\n",
    "            i.e. they are not allowed to play a move (this punishes them aswell as they will more likely lose!)\n",
    "\n",
    "            Here I follow the approach that the Agent is NOT able to repeat the move if it was invalid!\n",
    "            TODO: In that case, the \"valid\" variable is unnecessary\n",
    "            '''\n",
    "\n",
    "            # Opponent makes their turn\n",
    "            state = env.get_state('class' if type(opponent) in [MinimaxAgent] else 'list')\n",
    "            action = opponent.act(env if type(opponent) is MinimaxAgent else state)#.get_state_inverted())\n",
    "            if VERBOSE or i >= NUM_EPISODES[mode] - 2: print(f\"Opponents Action: {action}\")\n",
    "            valid, reward, finished = env.step(action, OPPONENT)\n",
    "            turns += 1\n",
    "            if valid == -1: invalid += 1\n",
    "            if VERBOSE or i >= NUM_EPISODES[mode] - 2: env.render_console()\n",
    "            if finished != -1: break\n",
    "        \n",
    "        # env.render_pretty()\n",
    "        \n",
    "        # Print the last game\n",
    "        if i == NUM_EPISODES[mode] and mode == 'EVAL':\n",
    "            VERBOSE = False\n",
    "\n",
    "        # Update opponent with old versions of the agent, i.e. make the agent play against itself\n",
    "        if UPDATE_OPPONENT and mode == 'TRAIN' and i % OPPONENT_UPDATE_FREQUENCY == 0 and i > BOOTSTRAP_EPISODES:\n",
    "            agent_class = type(agent)\n",
    "            opponent = agent_class(env=env, epsilon_max=0.1, epsilon_min = 0.1, device=device, options={'weights_init': agent})\n",
    "        \n",
    "        if DECAY_RANDOMNESS_OPPONENT and mode == 'TRAIN' and i % DECAY_RANDOMNESS_FREQUENCY:\n",
    "            if type(opponent) is MinimaxAgent:\n",
    "                opponent.decay_epsilon()\n",
    "\n",
    "        episode_str = f'\\n Winner of episode {i} was player {finished}.'\n",
    "        if finished == 1:\n",
    "            p1_score += 1\n",
    "        elif finished == 2:\n",
    "            p2_score += 1\n",
    "        else:\n",
    "            episode_str = f'Episode {i} ended in a draw.'\n",
    "\n",
    "        if VERBOSE or i == NUM_EPISODES[mode]:\n",
    "            if mode == 'EVAL': episode_str = '\\n Results of evaluation were:'\n",
    "            print(episode_str + f' P1 has {p1_score} wins, P2 has {p2_score} wins, and there were {i - p1_score - p2_score} draws.')\n",
    "            print('End state of the last game was:')\n",
    "            # env.render_console()\n",
    "\n",
    "        env.reset()\n",
    "\n",
    "    print('Average turns per episode ', turns / NUM_EPISODES[mode])\n",
    "    print('Average invalid moves per episode ', invalid / NUM_EPISODES[mode])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
