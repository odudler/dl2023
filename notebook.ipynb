{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base libraries\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Callable, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# ML libraries\n",
    "import torch\n",
    "\n",
    "# Local imports\n",
    "from board import ConnectFourField\n",
    "from env import Env\n",
    "from random_agent import RandomAgent\n",
    "from deep_q_agent import DeepQAgent\n",
    "import utils\n",
    "\n",
    "# Fix random seed\n",
    "utils.seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Environment and Agent\n",
    "env = Env()\n",
    "agent = DeepQAgent(env)\n",
    "\n",
    "AGENT = 1\n",
    "OPPONENT = 2\n",
    "NUM_EPISODES = {'train': 2000, 'eval': 100}\n",
    "VERBOSE = True\n",
    "MODES = ['train', 'eval']\n",
    "\n",
    "if VERBOSE: env.render_console()\n",
    "\n",
    "#Initialize Opponent Agent (This Agent is NOT trained)\n",
    "opponent = RandomAgent(env)\n",
    "\n",
    "for mode in MODES:\n",
    "    # Reset score counter\n",
    "    p1_score = 0\n",
    "    p2_score = 0\n",
    "\n",
    "    for i in range(1, NUM_EPISODES[mode] + 1):\n",
    "        # Initialize other variables\n",
    "        finished = -1\n",
    "\n",
    "        # Make it random who gets to start the game\n",
    "        # Set to true during the episode\n",
    "        agent_start = random.choice([True, False])\n",
    "        # Run one episode of the game\n",
    "        while finished == -1:\n",
    "            if finished != -1: break\n",
    "            # Agent makes a turn\n",
    "            if agent_start:\n",
    "                state = env.get_state()\n",
    "                action = agent.act(state)\n",
    "                if VERBOSE: print(f\"Agents Action: {action}\")\n",
    "                valid, reward, finished = agent.env.step(action, AGENT)\n",
    "                if VERBOSE: env.render_console()\n",
    "\n",
    "                # TODO: Here all the code for storing sequences in the buffer and learning/training the network would be!\n",
    "                if type(agent) is not RandomAgent and mode == 'train':\n",
    "                    agent.remember(state, action, reward, env.get_state(), finished)\n",
    "                    agent.optimize_model()\n",
    "                if finished != -1: break\n",
    "            else:\n",
    "                agent_start = True\n",
    "\n",
    "            # If move was invalid, repeat TODO: cumulate negative reward in this case!\n",
    "            '''\n",
    "            TODO: How to handle this whole \"invalid move\" situation in general,\n",
    "            1) Should we adapt the actionspace to only the valid actions? (hard..)\n",
    "            2) Punish the Agent for making a invalid move, but how to we represent that in the sequence?\n",
    "            -> Easiest way would probably just be to give negative reward and make the agent \"skip their move\",\n",
    "            i.e. they are not allowed to play a move (this punishes them aswell as they will more likely lose!)\n",
    "\n",
    "            Here I follow the approach that the Agent is NOT able to repeat the move if it was invalid!\n",
    "            TODO: In that case, the \"valid\" variable is unnecessary\n",
    "            '''\n",
    "\n",
    "            # Opponent makes their turn\n",
    "            action = opponent.act()\n",
    "            if VERBOSE: print(f\"Opponents Action: {action}\")\n",
    "            valid, reward, finished = opponent.env.step(action, OPPONENT)\n",
    "            if VERBOSE: env.render_console()\n",
    "            if finished != -1: break\n",
    "\n",
    "        episode_str = f'Winner of episode {i} was player {finished}.'\n",
    "        if finished == 1:\n",
    "            p1_score += 1\n",
    "        elif finished == 2:\n",
    "            p2_score += 1\n",
    "        else:\n",
    "            episode_str = f'Episode {i} ended in a draw.'\n",
    "\n",
    "        if VERBOSE or i == NUM_EPISODES[mode]:\n",
    "            if mode == 'eval': episode_str = 'Results of evaluation were:'\n",
    "            print(episode_str + f' P1 has {p1_score} wins, P2 has {p2_score} wins, and there were {i - p1_score - p2_score} draws.')\n",
    "            print('End state of the last game was:')\n",
    "            env.render_pretty()\n",
    "\n",
    "        env.reset()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
