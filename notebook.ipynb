{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reload_ext dotenv\n",
    "#%dotenv\n",
    "\n",
    "# ML libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Local imports\n",
    "from env import Env\n",
    "from agents.random_agent import RandomAgent\n",
    "from agents.minimax_agent import MinimaxAgent\n",
    "from agents.minimax_agent_old import OldMinimaxAgent\n",
    "# from agents.deep_q_agent import DeepQAgent\n",
    "from agents.deep_q_agent_modified import DeepQAgent\n",
    "from agents.cql_agent import CQLAgent\n",
    "import utils\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# INITIALIZATION #\n",
    "##################\n",
    "\n",
    "# Fix random seed\n",
    "utils.seed_everything(42, deterministic=False)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define player and opponent IDs\n",
    "AGENT = 1\n",
    "OPPONENT = 2\n",
    "\n",
    "# Define game environment, this should be passed to the agents and the trainer\n",
    "env = Env()\n",
    "\n",
    "# Define agent and opponent\n",
    "state_shape = env.get_state(state_type='boolean', player=AGENT).shape\n",
    "# agent = CQLAgent(env=env, state_size=state_shape, action_size=7, network_type='CNN')\n",
    "# agent.load_model('./saved_models/CQLAgent_CNN_25783.pt')\n",
    "# agent = CQLAgent(env=env, state_size=42, action_size=7, network_type='DDQN')\n",
    "agent = DeepQAgent(env=env, epsilon_max=1, epsilon_min=0.1, epsilon_decay=0.9999, device=device)\n",
    "opponent = MinimaxAgent(env=env, depth=3, epsilon=0.5, player=OPPONENT)\n",
    "replacement_agent = MinimaxAgent(env=env, depth=3, epsilon=0.5, player=OPPONENT)\n",
    "\n",
    "# Define options for training\n",
    "options = {\n",
    "           'UPDATE_OPPONENT': True,                         # Whether to enable self-play or not\n",
    "           'OPPONENT_UPDATE_FREQUENCY': 100,                # After how many episodes the opponent will be replaced by the current agent\n",
    "           'BOOTSTRAP_EPISODES': 0000,                      # During this time, the agent will not be replaced by itself\n",
    "           'DECAY_RANDOMNESS_OPPONENT': False,              # Decay randomness of the opponent. Use only if the opponent acts with some randomness\n",
    "           'DECAY_RANDOMNESS_FREQUENCY': 1000,              # Frequency of randomness decay\n",
    "           'REPLACE_FOR_EVALUATION': True,                  # Whether to replace the training model at the end with another evaluation model\n",
    "           'REPLACE_FOR_EVALUATION_BY': replacement_agent,  # Evalutation model to replace training model by\n",
    "           'AUTOSAVE': True,                                # Whether to save the model at certain intervals\n",
    "           'AUTOSAVE_TYPE': 'NUM_EPISODES',                 # One of [\"NUM_OPTIMIZATIONS\", \"NUM_EPISODES\"]\n",
    "           'AUTOSAVE_PERIOD': 1000,                         # After how many _ to save the model\n",
    "           }\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(env=env, agent=agent, opponent=opponent, agent_id=AGENT, opponent_id=OPPONENT, num_episodes={'TRAIN': 5000, 'EVAL': 100}, device=device, verbose=True, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Running episode 500 of 5000. Current win ratio of AGENT is 8.20%."
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m############\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# TRAINING #\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m############\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Train agent\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ninol\\dl2023\\trainer.py:160\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    158\u001b[0m     p2_score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# Optional periodic updates\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_periodic_updates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Reset board to empty\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32mc:\\Users\\ninol\\dl2023\\trainer.py:263\u001b[0m, in \u001b[0;36mTrainer.perform_periodic_updates\u001b[1;34m(self, mode, episode)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (BOOTSTRAP_EPISODES \u001b[38;5;129;01mand\u001b[39;00m episode \u001b[38;5;241m>\u001b[39m BOOTSTRAP_EPISODES \u001b[38;5;129;01mand\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mOPPONENT_UPDATE_FREQUENCY \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[0;32m    260\u001b[0m (\u001b[38;5;129;01mnot\u001b[39;00m BOOTSTRAP_EPISODES \u001b[38;5;129;01mand\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mOPPONENT_UPDATE_FREQUENCY \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# Update opponent with current version of the agent\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     agent_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent)\n\u001b[1;32m--> 263\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m(agent_class \u001b[38;5;129;01min\u001b[39;00m [CQLAgent, DeepQAgent])\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m agent_class \u001b[38;5;241m==\u001b[39m CQLAgent:\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopponent \u001b[38;5;241m=\u001b[39m agent_class(env\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, epsilon_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, epsilon_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDEVICE, network_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mnetwork_type, options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights_init\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent})\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "# Train agent\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: Ran episode 100 of 100. Ratios are [WINS: 33.00% | LOSSES: 67.00% | TIES: 0]                                                \n",
      "EVAL: Average turns per episode 10.58\n",
      "EVAL: Average invalid moves per episode 0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# QUICK EVALUATION #\n",
    "####################\n",
    "\n",
    "# Fix random seed\n",
    "utils.seed_everything(42, deterministic=False)\n",
    "\n",
    "# Define player and opponent IDs\n",
    "AGENT = 1\n",
    "OPPONENT = 2\n",
    "\n",
    "# Fix random seed\n",
    "utils.seed_everything(42, deterministic=False)\n",
    "\n",
    "eval_env = Env()\n",
    "# Load agent from save and set to eval mode\n",
    "state_shape = env.get_state(state_type='boolean', player=AGENT).shape\n",
    "agent = CQLAgent(env=eval_env, state_size=state_shape, action_size=7, network_type='CNN')\n",
    "agent.load_model('./saved_models/CQLAgent_CNN_60936.pt')\n",
    "\n",
    "# Define opponent\n",
    "minmax_opponent = MinimaxAgent(env=eval_env, depth=3, epsilon=0.5, player=OPPONENT)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(env=eval_env, agent=agent, opponent=opponent, agent_id=AGENT, opponent_id=OPPONENT, num_episodes={'TRAIN': 10000, 'EVAL': 100}, device=device, verbose=True, options=options)\n",
    "# Run evaluation\n",
    "trainer.eval(agent=agent, opponent=minmax_opponent, episodes=100, agent_start=None, print_last_n_games=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
