{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "# ML libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Local imports\n",
    "from env import Env\n",
    "from trainer import Trainer\n",
    "import utils\n",
    "# Training Agents\n",
    "from agents.cql_agent import CQLAgent\n",
    "from agents.deep_q_agent import DeepQAgent\n",
    "from agents.deep_q_agent_double_q import DDQAgent\n",
    "# Non-Training Agents\n",
    "from agents.minimax_agent import MinimaxAgent\n",
    "from agents.random_agent import RandomAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# INITIALIZATION #\n",
    "##################\n",
    "\n",
    "# Tweek these parameters for training and evaluation\n",
    "num_episodes = {'TRAIN': 2000, 'EVAL': 100}\n",
    "with_options = False\n",
    "# Agent\n",
    "agent_type = 'DDQAgent'         # DDQAgent, DeepQAgent, CQLAgent\n",
    "agent_network_type = 'FCNN'            # CNN, FCNN\n",
    "# Opponent\n",
    "opponent_type = 'DDQAgent'  # DDQAgent, DeepQAgent, CQLAgent, MinimaxAgent, RandomAgent\n",
    "opponent_network_type = 'FCNN'            # CNN, FCNN\n",
    "minimax_depth = 2\n",
    "minimax_epsilon = 0.3\n",
    "# Same for both agent and opponent\n",
    "epsilon_max = 1\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.9997\n",
    "\n",
    "# Fix random seed\n",
    "utils.seed_everything(43, deterministic=False)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Define player and opponent IDs\n",
    "AGENT = 1\n",
    "OPPONENT = 2\n",
    "\n",
    "# Define game environment, this should be passed to the agents and the trainer\n",
    "env = Env()\n",
    "\n",
    "# Define agent\n",
    "if agent_type == 'DDQAgent':\n",
    "    agent = DDQAgent(env=env, epsilon_max=epsilon_max, epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, network_type=agent_network_type, device=device)\n",
    "elif agent_type == 'DeepQAgent':\n",
    "    agent = DeepQAgent(env=env, epsilon_max=epsilon_max, epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, network_type=agent_network_type, device=device)\n",
    "elif agent_type == 'CQLAgent':\n",
    "    agent = CQLAgent(env=env, epsilon_max=epsilon_max, epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, network_type=agent_network_type, device=device)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown agent_type: {agent_type}\")\n",
    "# Define opponent\n",
    "if opponent_type == 'DDQAgent':\n",
    "    opponent = DDQAgent(env=env, epsilon_max=epsilon_max, epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, network_type=opponent_network_type, device=device)\n",
    "elif opponent_type == 'DeepQAgent':\n",
    "    opponent = DeepQAgent(env=env, epsilon_max=epsilon_max, epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, network_type=opponent_network_type, device=device)\n",
    "elif opponent_type == 'CQLAgent':\n",
    "    opponent = CQLAgent(env=env, epsilon_max=epsilon_max, epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, network_type=opponent_network_type, device=device)\n",
    "elif opponent_type == 'MinimaxAgent':\n",
    "    opponent = MinimaxAgent(env=env, depth=minimax_depth, epsilon=minimax_epsilon, player=OPPONENT)\n",
    "elif opponent_type == 'RandomAgent':\n",
    "    opponent = RandomAgent(env=env)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown opponent_type: {opponent_type}\")\n",
    "\n",
    "# Define options for training\n",
    "replacement_agent = None\n",
    "options = {\n",
    "           'UPDATE_OPPONENT': False,                         # Whether to enable self-play or not\n",
    "           'OPPONENT_UPDATE_FREQUENCY': 100,                # After how many episodes the opponent will be replaced by the current agent\n",
    "           'BOOTSTRAP_EPISODES': 7500,                      # During this time, the agent will not be replaced by itself\n",
    "           'DECAY_RANDOMNESS_OPPONENT': False,               # Decay randomness of the opponent. Use only if the opponent acts with some randomness\n",
    "           'DECAY_RANDOMNESS_FREQUENCY': 1000,              # Frequency of randomness decay\n",
    "           'REPLACE_FOR_EVALUATION': False,                  # Whether to replace the training model at the end with another evaluation model\n",
    "        #    'REPLACE_FOR_EVALUATION_BY': replacement_agent,  # Evalutation model to replace training model by\n",
    "           'AUTOSAVE': True,                                # Whether to save the model at certain intervals\n",
    "           'AUTOSAVE_TYPE': 'NUM_EPISODES',                 # One of [\"NUM_OPTIMIZATIONS\", \"NUM_EPISODES\"]\n",
    "           'AUTOSAVE_PERIOD': 1000,                         # After how many _ to save the model\n",
    "           }\n",
    "if not with_options: options = None\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(env=env, agent=agent, opponent=opponent, agent_id=AGENT, opponent_id=OPPONENT, num_episodes=num_episodes, device=device, verbose=True, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: Running episode 2000 of 2000. Agent won 224 times. Current win ratio of AGENT is 11.20%. Agent Parameters: Epsilon = 0.037009, Memory Size = 10000\n",
      "TRAIN: Average turns per episode 5.5955\n",
      "TRAIN: Average invalid moves per episode 0.0\n",
      "\n",
      "\n",
      "Model was saved in ./saved_models/ as CQLAgent_FCNN_10992.pt\n",
      "EVAL: Running episode 100 of 100. Agent won 14 times. Current win ratio of AGENT is 14.00%. Agent Parameters: Epsilon = 0.036954, Memory Size = 10000  \n",
      "EVAL: Average turns per episode 5.37\n",
      "EVAL: Average invalid moves per episode 0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# TRAINING #\n",
    "############\n",
    "\n",
    "# Train agent\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: CQLAgent_CNN_RS41.pt vs. CQLAgent_CNN_RS41.pt\n",
      "EVAL: Running episode 1000 of 1000. Ratios are [WINS: 55.20% | LOSSES: 44.20% | TIES: 0.60%]\n",
      "EVAL: Average turns per episode 12.743\n",
      "EVAL: Average invalid moves per episode 0.0\n",
      "\n",
      "\n",
      "Evaluating: CQLAgent_CNN_RS42.pt vs. CQLAgent_CNN_RS42.pt\n",
      "EVAL: Running episode 1000 of 1000. Ratios are [WINS: 51.70% | LOSSES: 47.40% | TIES: 0.90%]\n",
      "EVAL: Average turns per episode 13.364\n",
      "EVAL: Average invalid moves per episode 0.0\n",
      "\n",
      "\n",
      "Evaluating: CQLAgent_CNN_RS43.pt vs. CQLAgent_CNN_RS43.pt\n",
      "EVAL: Running episode 1000 of 1000. Ratios are [WINS: 50.90% | LOSSES: 47.10% | TIES: 2.00%]\n",
      "EVAL: Average turns per episode 12.481\n",
      "EVAL: Average invalid moves per episode 0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# AGENT EVALUATION #\n",
    "####################\n",
    "\n",
    "# To reproduce our results, you only need to set agent_type/opponent_type and agent_network_type/opponent_network_type\n",
    "\n",
    "# Tweek these parameters for evaluation\n",
    "episodes = 1000\n",
    "# Agent\n",
    "agent_type = 'CQLAgent'                 # DDQAgent, DeepQAgent, CQLAgent, MinimaxAgent, RandomAgent\n",
    "agent_network_type = 'CNN'              # CNN, FCNN\n",
    "# Opponent\n",
    "opponent_type = 'CQLAgent'              # DDQAgent, DeepQAgent, CQLAgent, MinimaxAgent, RandomAgent\n",
    "opponent_network_type = 'CNN'           # CNN, FCNN\n",
    "minimax_depth = 2\n",
    "minimax_epsilon = 0.3\n",
    "epsilon_max = 0.1 # Allow some randomness during evaluation\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.9997 # No decay during evaluation\n",
    "\n",
    "# Set random seed values (we chose 41, 42 and 43)\n",
    "random_seeds = [41, 42, 43]\n",
    "\n",
    "# You don't need to change anything below this for evaluation\n",
    "#############################################################\n",
    "for random_seed_nr in random_seeds:\n",
    "    # Fix random seed\n",
    "    utils.seed_everything(random_seed_nr, deterministic=False)\n",
    "\n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define player and opponent IDs\n",
    "    AGENT = 1\n",
    "    OPPONENT = 2\n",
    "\n",
    "    eval_env = Env()\n",
    "\n",
    "    # Load agent from save and set to eval mode\n",
    "    if agent_type == 'DDQAgent':\n",
    "        agent = DDQAgent(env=eval_env, epsilon_max=epsilon_max, epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, network_type=agent_network_type, device=device)\n",
    "    elif agent_type == 'DeepQAgent':\n",
    "        agent = DeepQAgent(env=eval_env, epsilon_max=epsilon_max, epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, network_type=agent_network_type, device=device)\n",
    "    elif agent_type == 'CQLAgent':\n",
    "        agent = CQLAgent(env=eval_env, epsilon_max=epsilon_max, epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, network_type=agent_network_type, device=device)\n",
    "    elif agent_type == 'MinimaxAgent':\n",
    "        agent = MinimaxAgent(env=eval_env, depth=minimax_depth, epsilon=minimax_epsilon, player=AGENT)\n",
    "    elif agent_type == 'RandomAgent':\n",
    "        agent = RandomAgent(env=eval_env)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown agent_type: {agent_type}\")\n",
    "\n",
    "    # Define opponent\n",
    "    if opponent_type == 'DDQAgent':\n",
    "        opponent = DDQAgent(env=eval_env, epsilon_max=epsilon_max, epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, network_type=opponent_network_type, device=device)\n",
    "    elif opponent_type == 'DeepQAgent':\n",
    "        opponent = DeepQAgent(env=eval_env, epsilon_max=epsilon_max, epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, network_type=opponent_network_type, device=device)\n",
    "    elif opponent_type == 'CQLAgent':\n",
    "        opponent = CQLAgent(env=eval_env, epsilon_max=epsilon_max, epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, network_type=opponent_network_type, device=device)\n",
    "    elif opponent_type == 'MinimaxAgent':\n",
    "        opponent = MinimaxAgent(env=eval_env, depth=minimax_depth, epsilon=minimax_epsilon, player=OPPONENT)\n",
    "    elif opponent_type == 'RandomAgent':\n",
    "        opponent = RandomAgent(env=eval_env)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown opponent_type: {opponent_type}\")\n",
    "\n",
    "    agent_string = f\"{agent_type}_{agent_network_type}_RS{random_seed_nr}.pt\" if agent_type in ['DDQAgent', 'CQLAgent', 'DeepQAgent'] else None\n",
    "    opponent_string = f\"{opponent_type}_{opponent_network_type}_RS{random_seed_nr}.pt\" if opponent_type in ['DDQAgent', 'CQLAgent', 'DeepQAgent'] else None\n",
    "\n",
    "    # Load pretrained models\n",
    "    if agent_type in ['DDQAgent', 'CQLAgent', 'DeepQAgent']:\n",
    "        agent.load_model(f'./dqn_cql_model/{agent_string}')\n",
    "    if opponent_type in ['DDQAgent', 'CQLAgent', 'DeepQAgent']:\n",
    "        opponent.load_model(f'./dqn_cql_model/{opponent_string}')\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(env=eval_env, agent=agent, opponent=opponent, agent_id=AGENT, opponent_id=OPPONENT, device=device, verbose=True)\n",
    "\n",
    "    # Run evaluation\n",
    "    print(f\"Evaluating: {agent_string if agent_string else agent_type + str(random_seed_nr)} vs. {opponent_string if opponent_string else opponent_type + str(random_seed_nr)}\")\n",
    "    trainer.eval(agent=agent, opponent=opponent, episodes=episodes, agent_start=True, print_last_n_games=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.60\\pm2.29\n"
     ]
    }
   ],
   "source": [
    "# Mean and STD calculation\n",
    "import numpy as np\n",
    "\n",
    "data = [55.2,\n",
    "        51.7,\n",
    "        50.9]\n",
    "\n",
    "assert len(data) == 3, \"Check data!\"\n",
    "mean = np.mean(data)\n",
    "std = np.std(data, ddof = 1) # Using Sample Standard Deviation\n",
    "\n",
    "print(f\"{mean:.2f}\\pm{std:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
