{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "# Base libraries\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Callable, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from typing import Union\n",
    "\n",
    "# ML libraries\n",
    "import torch\n",
    "\n",
    "# Local imports\n",
    "from board import ConnectFourField\n",
    "from env import Env\n",
    "from agents.random_agent import RandomAgent\n",
    "from agents.deep_q_agent import DeepQAgent\n",
    "from agents.cql_agent import CQLAgent\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TRAIN: Running episode 1000 of 1000.                                                                                   TRAIN: Running episode 646 of 1000.\n",
      " Winner of episode 1000 was player 1. P1 has 574 wins, P2 has 426 wins, and there were 0 draws.\n",
      "Average turns per episode  17.449\n",
      "Average invalid moves per episode  16.112\n",
      "Changing agent to random agent for evaluation.\n",
      " EVAL: Running episode 100 of 100.Agents Action: 2                                                                      \n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|1|0|0|0|0|\n",
      "===============\n",
      "Opponents Action: 1\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|2|1|0|0|0|0|\n",
      "===============\n",
      "Agents Action: 0\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|1|2|1|0|0|0|0|\n",
      "===============\n",
      "Opponents Action: 3\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|1|2|1|2|0|0|0|\n",
      "===============\n",
      "Agents Action: 2\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|1|0|0|0|0|\n",
      "|1|2|1|2|0|0|0|\n",
      "===============\n",
      "Opponents Action: 2\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|2|0|0|0|0|\n",
      "|0|0|1|0|0|0|0|\n",
      "|1|2|1|2|0|0|0|\n",
      "===============\n",
      "Agents Action: 4\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|2|0|0|0|0|\n",
      "|0|0|1|0|0|0|0|\n",
      "|1|2|1|2|1|0|0|\n",
      "===============\n",
      "Opponents Action: 4\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|2|0|0|0|0|\n",
      "|0|0|1|0|2|0|0|\n",
      "|1|2|1|2|1|0|0|\n",
      "===============\n",
      "Agents Action: 4\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|2|0|1|0|0|\n",
      "|0|0|1|0|2|0|0|\n",
      "|1|2|1|2|1|0|0|\n",
      "===============\n",
      "Opponents Action: 4\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|2|0|0|\n",
      "|0|0|2|0|1|0|0|\n",
      "|0|0|1|0|2|0|0|\n",
      "|1|2|1|2|1|0|0|\n",
      "===============\n",
      "Agents Action: 4\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|1|0|0|\n",
      "|0|0|0|0|2|0|0|\n",
      "|0|0|2|0|1|0|0|\n",
      "|0|0|1|0|2|0|0|\n",
      "|1|2|1|2|1|0|0|\n",
      "===============\n",
      "Opponents Action: 5\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|1|0|0|\n",
      "|0|0|0|0|2|0|0|\n",
      "|0|0|2|0|1|0|0|\n",
      "|0|0|1|0|2|0|0|\n",
      "|1|2|1|2|1|2|0|\n",
      "===============\n",
      "Agents Action: 6\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|1|0|0|\n",
      "|0|0|0|0|2|0|0|\n",
      "|0|0|2|0|1|0|0|\n",
      "|0|0|1|0|2|0|0|\n",
      "|1|2|1|2|1|2|1|\n",
      "===============\n",
      "Opponents Action: 1\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|1|0|0|\n",
      "|0|0|0|0|2|0|0|\n",
      "|0|0|2|0|1|0|0|\n",
      "|0|2|1|0|2|0|0|\n",
      "|1|2|1|2|1|2|1|\n",
      "===============\n",
      "Agents Action: 6\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|1|0|0|\n",
      "|0|0|0|0|2|0|0|\n",
      "|0|0|2|0|1|0|0|\n",
      "|0|2|1|0|2|0|1|\n",
      "|1|2|1|2|1|2|1|\n",
      "===============\n",
      "Opponents Action: 5\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|1|0|0|\n",
      "|0|0|0|0|2|0|0|\n",
      "|0|0|2|0|1|0|0|\n",
      "|0|2|1|0|2|2|1|\n",
      "|1|2|1|2|1|2|1|\n",
      "===============\n",
      "Agents Action: 6\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|1|0|0|\n",
      "|0|0|0|0|2|0|0|\n",
      "|0|0|2|0|1|0|1|\n",
      "|0|2|1|0|2|2|1|\n",
      "|1|2|1|2|1|2|1|\n",
      "===============\n",
      "Opponents Action: 2\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|1|0|0|\n",
      "|0|0|2|0|2|0|0|\n",
      "|0|0|2|0|1|0|1|\n",
      "|0|2|1|0|2|2|1|\n",
      "|1|2|1|2|1|2|1|\n",
      "===============\n",
      "Agents Action: 6\n",
      "_______________\n",
      "|0|0|0|0|0|0|0|\n",
      "|0|0|0|0|1|0|0|\n",
      "|0|0|2|0|2|0|1|\n",
      "|0|0|2|0|1|0|1|\n",
      "|0|2|1|0|2|2|1|\n",
      "|1|2|1|2|1|2|1|\n",
      "===============\n",
      "\n",
      " Results of evaluation were: P1 has 64 wins, P2 has 36 wins, and there were 0 draws.\n",
      "Average turns per episode  16.89\n",
      "Average invalid moves per episode  16.6\n"
     ]
    }
   ],
   "source": [
    "# Fix random seed\n",
    "utils.seed_everything(42, deterministic=True)\n",
    "\n",
    "# Use GPU if available\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#Initialize Environment and Agent\n",
    "env = Env()\n",
    "agent = DeepQAgent(env, epsilon_max = 1, epsilon_min = 0.01, device=device)\n",
    "\n",
    "AGENT = 1\n",
    "OPPONENT = 2\n",
    "NUM_EPISODES = {'TRAIN': 1000, 'EVAL': 100} # Define number of episodes for training and evaluation \n",
    "VERBOSE = False\n",
    "MODES = ['TRAIN', 'EVAL'] # Loop modes\n",
    "UPDATE_OPPONENT = True # Wether to update opponent with current player\n",
    "OPPONENT_UPDATE_FREQUENCY = 50 # Update opponent with current player every n episodes\n",
    "BOOTSTRAP_EPISODES = 100 # Episodes where opponent is just random agent\n",
    "\n",
    "if VERBOSE: env.render_console()\n",
    "\n",
    "#Initialize Opponent Agent (This Agent is NOT trained)\n",
    "opponent = RandomAgent(env)\n",
    "\n",
    "for mode in MODES:\n",
    "    # Reset score counter\n",
    "    p1_score = 0\n",
    "    p2_score = 0\n",
    "\n",
    "    turns = 0\n",
    "    invalid = 0\n",
    "\n",
    "    if mode == 'EVAL' and type(opponent) is not RandomAgent:\n",
    "        print('Changing agent to random agent for evaluation.')\n",
    "        # For evaluation, make opponent a random agent again\n",
    "        opponent = RandomAgent(env)\n",
    "\n",
    "    for i in range(1, NUM_EPISODES[mode] + 1):\n",
    "        # Clean up terminal line \n",
    "        if i % 100 == 0: print('\\r', '                                                                                                                       ', end='')\n",
    "        # Print current episode\n",
    "        print('\\r', f'{mode}: Running episode {i} of {NUM_EPISODES[mode]}.', end='')\n",
    "\n",
    "        # Print the last game\n",
    "        if i == NUM_EPISODES[mode] and mode == 'EVAL':\n",
    "            VERBOSE = True\n",
    "\n",
    "        # Initialize other variables\n",
    "        finished = -1\n",
    "\n",
    "        # Make it random who gets to start the game\n",
    "        # Set to true during the episode\n",
    "        agent_start = random.choice([True, False])\n",
    "        # Run one episode of the game\n",
    "        while finished == -1:\n",
    "            if finished != -1: break\n",
    "            # Agent makes a turn\n",
    "            if agent_start:\n",
    "                state = env.get_state()\n",
    "                action = agent.act(state)\n",
    "                if VERBOSE: print(f\"Agents Action: {action}\")\n",
    "                valid, reward, finished = agent.env.step(action, AGENT)\n",
    "                turns += 1\n",
    "                if not valid: invalid += 1\n",
    "                if VERBOSE: env.render_console()\n",
    "\n",
    "                # TODO: Here all the code for storing sequences in the buffer and learning/training the network would be!\n",
    "                if type(agent) is not RandomAgent and mode == 'TRAIN':\n",
    "                    agent.remember(state, action, reward, env.get_state(), finished)\n",
    "                    agent.optimize_model()\n",
    "                if finished != -1: break\n",
    "            else:\n",
    "                agent_start = True\n",
    "\n",
    "            # If move was invalid, repeat TODO: cumulate negative reward in this case!\n",
    "            '''\n",
    "            TODO: How to handle this whole \"invalid move\" situation in general,\n",
    "            1) Should we adapt the actionspace to only the valid actions? (hard..)\n",
    "            2) Punish the Agent for making a invalid move, but how to we represent that in the sequence?\n",
    "            -> Easiest way would probably just be to give negative reward and make the agent \"skip their move\",\n",
    "            i.e. they are not allowed to play a move (this punishes them aswell as they will more likely lose!)\n",
    "\n",
    "            Here I follow the approach that the Agent is NOT able to repeat the move if it was invalid!\n",
    "            TODO: In that case, the \"valid\" variable is unnecessary\n",
    "            '''\n",
    "\n",
    "            # Opponent makes their turn\n",
    "            action = opponent.act(env.get_state_inverted())\n",
    "            if VERBOSE: print(f\"Opponents Action: {action}\")\n",
    "            valid, reward, finished = opponent.env.step(action, OPPONENT)\n",
    "            turns += 1\n",
    "            if not valid: invalid += 1\n",
    "            if VERBOSE: env.render_console()\n",
    "            if finished != -1: break\n",
    "        \n",
    "        # Print the last game\n",
    "        if i == NUM_EPISODES[mode] and mode == 'EVAL':\n",
    "            VERBOSE = False\n",
    "\n",
    "        # Update opponent with old versions of the agent, i.e. make the agent play against itself\n",
    "        if UPDATE_OPPONENT and mode == 'TRAIN' and i % OPPONENT_UPDATE_FREQUENCY == 0 and i > BOOTSTRAP_EPISODES:\n",
    "            agent_class = type(agent)\n",
    "            opponent = agent_class(env=env, epsilon_max=0.5, device=device, options={'weights_init': agent})\n",
    "\n",
    "        episode_str = f'\\n Winner of episode {i} was player {finished}.'\n",
    "        if finished == 1:\n",
    "            p1_score += 1\n",
    "        elif finished == 2:\n",
    "            p2_score += 1\n",
    "        else:\n",
    "            episode_str = f'Episode {i} ended in a draw.'\n",
    "\n",
    "        if VERBOSE or i == NUM_EPISODES[mode]:\n",
    "            if mode == 'EVAL': episode_str = '\\n Results of evaluation were:'\n",
    "            print(episode_str + f' P1 has {p1_score} wins, P2 has {p2_score} wins, and there were {i - p1_score - p2_score} draws.')\n",
    "            # print('End state of the last game was:')\n",
    "            # env.render_console()\n",
    "\n",
    "        env.reset()\n",
    "\n",
    "    print('Average turns per episode ', turns / NUM_EPISODES[mode])\n",
    "    print('Average invalid moves per episode ', invalid / NUM_EPISODES[mode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
