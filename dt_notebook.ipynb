{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base libraries\n",
    "from collections import namedtuple, deque\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Callable, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ML libraries\n",
    "import torch\n",
    "\n",
    "# Local imports\n",
    "from env import Env\n",
    "from agents.random_agent import RandomAgent\n",
    "from agents.deep_q_agent import DeepQAgent\n",
    "from agents.cql_agent import CQLAgent\n",
    "from agents.minimax_agent import MinimaxAgent\n",
    "import utils\n",
    "import transformer_utils\n",
    "\n",
    "# Fix random seed\n",
    "utils.seed_everything()\n",
    "\n",
    "# Import Transformers\n",
    "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''2 Options: \n",
    "        Generate Trajectories now\n",
    "        Load existing Trajectories from a file\n",
    "'''\n",
    "GENERATE_TRAJECTORIES =  False #Set if you want to load from file or generate them here\n",
    "\n",
    "# NOTE: Filename style: {agent_name}{input_params}_vs_{opponent_name}{input_params}_num{NUM_TRAJ}.pkl\n",
    "# file_to_load_from = '100k_minimax_02.pkl' #Define file from where to load the trajectories\n",
    "# file_to_store_to = '100k_random_traces.pkl' #Define file to where we should store the generated trajectories\n",
    "file_to_load_from = '100k_random_traces.pkl'\n",
    "\n",
    "AGENT = 1\n",
    "OPPONENT = 2\n",
    "# Number of created trajectories\n",
    "NUM_TRAJ = 100_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if GENERATE_TRAJECTORIES:\n",
    "    # Generate T_offline (offline Trajectories for the offline initialization of the replay buffer)\n",
    "    # NOTE: alternative would be to initialize with either Random Trajs, or with Traj.s from DQN Agent / CQL Agent\n",
    "    # NOTE: Corresponding paper: https://arxiv.org/pdf/2202.05607.pdf\n",
    "\n",
    "    \n",
    "    offline_trajectories = []\n",
    "\n",
    "    #Initialize Environment and Agent\n",
    "    env = Env()\n",
    "    # agent = MinimaxAgent(env, depth=2, epsilon=0.2, player=1)\n",
    "    # opponent = MinimaxAgent(env, depth=2, epsilon=0.2, player=2)\n",
    "    agent = MinimaxAgent(env, depth=2, epsilon=1.0, player=1)\n",
    "    opponent = MinimaxAgent(env, depth=2, epsilon=1.0, player=2)\n",
    "\n",
    "    agent_won = 0\n",
    "    agent_lost = 0\n",
    "\n",
    "    for i in tqdm(range(0, NUM_TRAJ)):\n",
    "\n",
    "        # Initialize other variables\n",
    "        finished = -1\n",
    "\n",
    "        #Initialize fields for trajectory\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "\n",
    "        # Make it random who gets to start the game\n",
    "        # Set to true during the episode\n",
    "        agent_start = random.choice([True, False])\n",
    "        # Run one episode of the game\n",
    "        while finished == -1:\n",
    "            # Agent makes a turn\n",
    "            if agent_start:\n",
    "                state = env.get_state()\n",
    "                #action = agent.act(state)\n",
    "                action = agent.act(env.field)\n",
    "                valid, reward, finished = agent.env.step(action, AGENT)\n",
    "                #env.render_pretty()\n",
    "                \n",
    "                \n",
    "                # Update current Trajectory\n",
    "                #Flatten the state to be a 42-entry 1 dimensional array\n",
    "                states.append(np.ravel(state))\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "\n",
    "                if finished != -1:\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                agent_start = True\n",
    "\n",
    "            # Opponent makes their turn\n",
    "            #action = opponent.act(env.get_state_inverted())\n",
    "            action = opponent.act(env.field)\n",
    "            valid, _, finished = opponent.env.step(action, OPPONENT)\n",
    "\n",
    "            if finished != -1: \n",
    "                if finished == OPPONENT:\n",
    "                    #TODO this is hacky, if this happens the opponent won and we need to set the last reward for the agent..\n",
    "                    rewards[-1] = -1\n",
    "                break\n",
    "\n",
    "        # The Flag if the Episode is finished is False for n-1 steps and True for the last step obviously..\n",
    "        dones = ([False] * (len(rewards)-1)) + [True]\n",
    "    \n",
    "        assert len(states) == len(actions)\n",
    "        assert len(actions) == len(rewards)\n",
    "        assert len(dones) == len(rewards)\n",
    "        length = len(states)\n",
    "        RTGs = transformer_utils.calculate_RTG(rewards)\n",
    "        traj = [length, states, actions, rewards, RTGs, dones]\n",
    "        offline_trajectories.append(traj)\n",
    "\n",
    "        #Keep track of statistics\n",
    "        if finished == 1:\n",
    "            agent_won+=1\n",
    "        elif finished == 2:\n",
    "            agent_lost+=1\n",
    "\n",
    "        if i % NUM_TRAJ/3 == 0:\n",
    "            print(f\"Episode: {i}, RTG (start) : {RTGs[0]}, length: {length}, reward end: {rewards[-1]}, done: {dones[-1]}\")\n",
    "            print(f\"Score: Agent {agent_won} - {agent_lost} Opponent\")\n",
    "            env.render_pretty()\n",
    "            # print(traj)\n",
    "        # if i % 100 == 0:\n",
    "        #     print(i)\n",
    "\n",
    "        env.reset()\n",
    "        \n",
    "    # Final score\n",
    "    print(f\"Score: Agent {agent_won} - {agent_lost} Opponent. There were {NUM_TRAJ - agent_won - agent_lost} Ties\")\n",
    "\n",
    "    # Sort offline buffer such that the order is descending in RTGs\n",
    "    # [4][0] is the first RTG of the trajectory\n",
    "    sorted_offline_trajectories = sorted(offline_trajectories, key=lambda x: x[4][0], reverse = True)\n",
    "\n",
    "\n",
    "    #TODO: We currently only use one side of the table for generating trajectories, we could generate twice as many by also storing the trajectories from the \"enemys POV\"\n",
    "\n",
    "if GENERATE_TRAJECTORIES:\n",
    "    # HERE WE STORE TRAJECTORIES THAT WE PRODUCED ABOVE, OR ALREADY PRODUCED EARLIER!\n",
    "\n",
    "    #Safety mechanism, to show if this specific file we want to write to doesn't already exist, and we lose our data.\n",
    "    #NOTE: just comment this check out if you want to overwrite a file..\n",
    "    if os.path.exists('trajectories/' + file_to_store_to):\n",
    "        raise FileExistsError(f\"File {file_to_store_to} already exists!\")\n",
    "    \n",
    "    #Create Trajectories folder if in-existent\n",
    "    if not os.path.exists('trajectories'):\n",
    "        os.makedirs('trajectories')\n",
    "    # Store data within specified file\n",
    "    with open(\"trajectories/\" + file_to_store_to, 'wb') as f:\n",
    "        pickle.dump(sorted_offline_trajectories, f)\n",
    "\n",
    "else:\n",
    "    #Check if the given file exists\n",
    "    if not os.path.exists('trajectories/' + file_to_load_from):\n",
    "        raise FileExistsError(f\"File {file_to_load_from} doesn't exist!\")\n",
    "    \n",
    "    #Load Trajectories from specified file:\n",
    "    with open('trajectories/' + file_to_load_from, 'rb') as f:\n",
    "        sorted_offline_trajectories = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, [array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 2]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 2, 0, 2, 2])], [0, 6, 1], [0.05, 0.05, -1], [-0.9, -0.95, -1], [False, False, True]]\n"
     ]
    }
   ],
   "source": [
    "print(sorted_offline_trajectories[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, [array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 2, 0, 0, 1, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 2, 2, 0, 0, 1, 0])], [5, 5, 5], [0.05, 0.05, -1], [-0.9, -0.95, -1], [False, False, True]]\n",
      "[3, [array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 2, 0, 0, 1, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 2, 2, 0, 0, 1, 0])], [[0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1, 0]], [0.05, 0.05, -1], [-0.9, -0.95, -1], [False, False, True]]\n"
     ]
    }
   ],
   "source": [
    "# convert the actions inside the \"sorted_offline_trajectories\"\n",
    "print(sorted_offline_trajectories[-1])\n",
    "\n",
    "#test_traj = [[4, 4, [1,2,3,4]]]\n",
    "\n",
    "for i in range(len(sorted_offline_trajectories)):\n",
    "    traj = sorted_offline_trajectories[i]\n",
    "    actions = traj[2]\n",
    "    new_actions = []\n",
    "    for a in actions:\n",
    "        new_action = [0,0,0,0,0,0,0]\n",
    "        new_action[a] = 1\n",
    "        new_actions.append(new_action)\n",
    "    sorted_offline_trajectories[i][2] = new_actions\n",
    "\n",
    "print(sorted_offline_trajectories[-1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Episode length: 21\n",
      "70000\n"
     ]
    }
   ],
   "source": [
    "# We have a dataset containing all the offline collected data\n",
    "# Using this tutorial: https://huggingface.co/blog/train-decision-transformers\n",
    "# We convert the dataset into data ready for the decision transformer\n",
    "FRACTION = 0.7\n",
    "N = int(FRACTION * NUM_TRAJ)\n",
    "dataset = sorted_offline_trajectories[:N]\n",
    "lengths = [x[0] for x in dataset]\n",
    "max_episode_length = max(lengths)\n",
    "print(f\"Maximum Episode length: {max_episode_length}\")\n",
    "print(len(dataset))\n",
    "\n",
    "# Constants represent the position of the value within a trajectory-array\n",
    "# trajectory structure in the dataset: traj = [length, states, actions, rewards, RTGs, dones]\n",
    "LENGTH = 0\n",
    "STATES = 1\n",
    "ACTIONS = 2\n",
    "REWARDS = 3\n",
    "RTGs = 4\n",
    "DONES = 5\n",
    "\n",
    "class DecisionTransformerGymDataCollator:\n",
    "    return_tensors: str = \"pt\"\n",
    "    max_len: int = 21 #subsets of the episode we use for training\n",
    "    state_dim: int = 42  # size of state space\n",
    "    act_dim: int = 7  # size of action space\n",
    "    max_ep_len: int = 42 # max episode length in the dataset TODO: is this the correct value?\n",
    "    #scale: float = 1000.0  # normalization of rewards/returns\n",
    "    n_traj: int = N # to store the number of trajectories in the dataset\n",
    "\n",
    "    def __init__(self, dataset) -> None:\n",
    "        self.act_dim = 7\n",
    "        self.state_dim = 42\n",
    "        self.dataset = dataset\n",
    "        self.n_traj = len(dataset)\n",
    "\n",
    "    def _discount_cumsum(self, x, gamma):\n",
    "        discount_cumsum = np.zeros_like(x)\n",
    "        discount_cumsum[-1] = x[-1]\n",
    "        for t in reversed(range(x.shape[0] - 1)):\n",
    "            discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
    "        return discount_cumsum\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch_size = len(features)\n",
    "        # this is a bit of a hack to be able to sample of a non-uniform distribution\n",
    "        batch_inds = np.random.choice(np.arange(self.n_traj), size=batch_size, replace=True)\n",
    "        \n",
    "        # a batch of dataset features\n",
    "        s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
    "        \n",
    "        for ind in batch_inds:\n",
    "            # Select trajectory at given index\n",
    "            feature = self.dataset[int(ind)]\n",
    "            #set si randomly to start somewhere in the sequence (but at least max_len steps before the end..)\n",
    "            length = self.dataset[ind][LENGTH]\n",
    "            if length <= self.max_len:\n",
    "                si = 0\n",
    "            else:\n",
    "                # we should just start from the end then, because we have sparse rewards...\n",
    "                #si = random.randint(0, length - self.max_len)\n",
    "                #NOTE: this case should never occur anyway if we work with a window size of the maximum possible episode length..\n",
    "                si = max(0, length - self.max_len - 1)\n",
    "\n",
    "            # get sequences from dataset\n",
    "            s.append(np.array(feature[STATES][si : si + self.max_len]).reshape(1, -1, self.state_dim))\n",
    "            a.append(np.array(feature[ACTIONS][si : si + self.max_len]).reshape(1, -1, self.act_dim))\n",
    "            r.append(np.array(feature[REWARDS][si : si + self.max_len]).reshape(1, -1, 1))\n",
    "\n",
    "            d.append(np.array(feature[DONES][si : si + self.max_len]).reshape(1, -1))\n",
    "            timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "            timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
    "            rtg.append(\n",
    "                self._discount_cumsum(np.array(feature[REWARDS][si:]), gamma=1.0)[\n",
    "                    : s[-1].shape[1]   # TODO check the +1 removed here\n",
    "                ].reshape(1, -1, 1)\n",
    "            )\n",
    "            if rtg[-1].shape[1] < s[-1].shape[1]:\n",
    "                print(\"if true\")\n",
    "                rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
    "\n",
    "            # padding and state\n",
    "            tlen = s[-1].shape[1]\n",
    "            s[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, self.state_dim)), s[-1]], axis=1)\n",
    "            a[-1] = np.concatenate(\n",
    "                [np.ones((1, self.max_len - tlen, self.act_dim)) * -10.0, a[-1]],\n",
    "                axis=1,\n",
    "            )\n",
    "            r[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), r[-1]], axis=1)\n",
    "            d[-1] = np.concatenate([np.ones((1, self.max_len - tlen)) * 2, d[-1]], axis=1)\n",
    "            rtg[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), rtg[-1]], axis=1)\n",
    "            timesteps[-1] = np.concatenate([np.zeros((1, self.max_len - tlen)), timesteps[-1]], axis=1)\n",
    "            mask.append(np.concatenate([np.zeros((1, self.max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
    "\n",
    "        # Converting\n",
    "        s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
    "        a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
    "        r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
    "        d = torch.from_numpy(np.concatenate(d, axis=0))\n",
    "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()\n",
    "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()\n",
    "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
    "\n",
    "        # This is how the trajectories are returned for the transformer to learn with them\n",
    "        return {\n",
    "            \"states\": s,\n",
    "            \"actions\": a,\n",
    "            \"rewards\": r,\n",
    "            \"returns_to_go\": rtg,\n",
    "            \"timesteps\": timesteps,\n",
    "            \"attention_mask\": mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This code is 1:1 from the Huggingface blog, don't know if the loss is correct for our usecase\n",
    "# class TrainableDT(DecisionTransformerModel):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "\n",
    "#     def forward(self, **kwargs):\n",
    "#         output = super().forward(**kwargs)\n",
    "#         # add the DT loss\n",
    "#         action_preds = output[1]\n",
    "#         action_targets = kwargs[\"actions\"]\n",
    "#         attention_mask = kwargs[\"attention_mask\"]\n",
    "#         act_dim = action_preds.shape[2]\n",
    "#         action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "#         action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "#         #print(f\"action_preds: {action_preds} and action_targets: {action_targets}, length: {len(action_preds)}\")\n",
    "        \n",
    "#         #loss = torch.mean((action_preds - action_targets) ** 2)\n",
    "#         criterion = torch.nn.CrossEntropyLoss()\n",
    "#         loss = criterion(action_preds, action_targets)\n",
    "\n",
    "#         return {\"loss\": loss}\n",
    "\n",
    "#     def original_forward(self, **kwargs):\n",
    "#         return super().forward(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output/\",\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=30, # 120\n",
    "    per_device_train_batch_size=64, # 64\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=0.25,\n",
    ")\n",
    "\n",
    "# Initialize and then train the model\n",
    "collator = DecisionTransformerGymDataCollator(dataset)\n",
    "config = DecisionTransformerConfig(state_dim=collator.state_dim, act_dim=collator.act_dim)\n",
    "model = transformer_utils.TrainableDT(config)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, [array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 0, 2, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "       1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 0, 2, 2, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "       1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 1, 1, 0, 2, 2, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "       1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "       1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 0, 2]), array([0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "       1, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 2, 2, 0, 2]), array([0, 1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "       1, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 2, 2, 1, 2]), array([0, 1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "       1, 0, 0, 0, 0, 0, 2, 2, 2, 0, 2, 0, 1, 1, 1, 1, 2, 2, 1, 2]), array([0, 1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "       1, 2, 0, 1, 0, 0, 2, 2, 2, 0, 2, 0, 1, 1, 1, 1, 2, 2, 1, 2])], [[1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0]], [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, -1], [-0.5, -0.55, -0.6000000000000001, -0.65, -0.7, -0.75, -0.8, -0.85, -0.9, -0.95, -1], [False, False, False, False, False, False, False, False, False, False, True]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[-1])\n",
    "# traj structure: [length, states, actions, rewards, RTGs, dones]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the model trained above...\n",
    "# trainer.save_model(\"dt_model/final_minimax_offline\")\n",
    "\n",
    "# Path to your saved model\n",
    "model_path = 'dt_model/final_random_offline'\n",
    "\n",
    "# Load the model\n",
    "model = transformer_utils.TrainableDT.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function, given a 7 elemenent array corresponding to an action, gives the int of the corresponding taken action :)\n",
    "\n",
    "\n",
    "# another util function that returns index of maximum in array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This cell is supposed to evaluate the Decision Transformer against a MinimaxAgent (or RandomAgent) for a fixed amount of Episodes\n",
    "# model = model.to(\"cpu\")\n",
    "# env = Env()\n",
    "# max_ep_len = max_episode_length\n",
    "# print(f\"Maximum Episode Length is {max_ep_len}\")\n",
    "# device = \"cpu\"\n",
    "# TARGET_RETURN = 2\n",
    "# NUM_EVAL = 100\n",
    "\n",
    "# opponent = MinimaxAgent(env, depth=2, epsilon=0.0, player=2)\n",
    "\n",
    "# games_won = 0\n",
    "# games_lost = 0\n",
    "\n",
    "# for ii in range(0,NUM_EVAL):\n",
    "\n",
    "#     state_dim = env.field.num_columns * env.field.num_rows\n",
    "#     act_dim = 7\n",
    "#     # Create the decision transformer model\n",
    "#     episode_return, episode_length = 0, 0\n",
    "#     env.reset()\n",
    "#     state = np.array(env.get_state()).flatten()\n",
    "#     target_return = torch.tensor(TARGET_RETURN, device=device, dtype=torch.float32).reshape(1, 1)\n",
    "#     states = torch.from_numpy(state).reshape(1, state_dim).to(device=device, dtype=torch.float32)\n",
    "#     actions = torch.zeros((0, act_dim), device=device, dtype=torch.float32)\n",
    "#     rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
    "\n",
    "#     timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)\n",
    "#     for t in range(max_ep_len):\n",
    "#         actions = torch.cat([actions, torch.zeros((1, act_dim), device=device)], dim=0)\n",
    "#         rewards = torch.cat([rewards, torch.zeros(1, device=device)])\n",
    "\n",
    "#         action = transformer_utils.get_action(\n",
    "#             model,\n",
    "#             states,\n",
    "#             actions,\n",
    "#             rewards,\n",
    "#             target_return,\n",
    "#             timesteps,\n",
    "#         )\n",
    "#         actions[-1] = action\n",
    "#         action = action.detach().cpu().numpy()\n",
    "#         #print(f\"Action chosen: {action}\")\n",
    "#         action_taken = np.argmax(action)\n",
    "#         #print(action_taken)\n",
    "\n",
    "#         #state, reward, done, _ = env.step(action)\n",
    "#         # valid, reward, finished = env.step(int(action), AGENT)\n",
    "#         valid, reward, finished = env.step(int(action_taken), AGENT)\n",
    "#         #print(f\"My reward: {reward}, move was valid: {valid}, is game finished: {finished}\")\n",
    "#         #env.render_console()\n",
    "#         state = np.array(env.get_state()).flatten()\n",
    "#         #env.render_pretty()\n",
    "\n",
    "#         if finished != -1:\n",
    "#             if finished == 1:\n",
    "#                 games_won += 1\n",
    "#             if finished == 2:\n",
    "#                 games_lost += 1\n",
    "#             break\n",
    "#         opp_action = opponent.act(env.field)\n",
    "#         #print(f\"Opponent Action: {opp_action}\")\n",
    "#         #print(env.field.field)\n",
    "#         valid, _, finished = env.step(int(opp_action), OPPONENT)\n",
    "#         state = np.array(env.get_state()).flatten()\n",
    "\n",
    "#         cur_state = torch.from_numpy(state).to(device=device).reshape(1, state_dim)\n",
    "#         states = torch.cat([states, cur_state], dim=0)\n",
    "#         rewards[-1] = reward\n",
    "\n",
    "#         pred_return = target_return[0, -1] - reward\n",
    "#         target_return = torch.cat([target_return, pred_return.reshape(1, 1)], dim=1)\n",
    "#         timesteps = torch.cat([timesteps, torch.ones((1, 1), device=device, dtype=torch.long) * (t + 1)], dim=1)\n",
    "\n",
    "#         episode_return += reward\n",
    "#         episode_length += 1\n",
    "\n",
    "#         if finished != -1:\n",
    "#             if finished == 1:\n",
    "#                 games_won += 1\n",
    "#             if finished == 2:\n",
    "#                 games_lost += 1\n",
    "#             break\n",
    "#     #env.render_pretty()\n",
    "\n",
    "# print(f\"Score: Agent {games_won} - {games_lost} Opponent. There were {NUM_EVAL - games_lost - games_won} Ties\")\n",
    "#     #env.render_pretty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished = -1\n",
    "env = Env()\n",
    "agent = MinimaxAgent(env, depth=4, epsilon=0, player=2)\n",
    "opponent = MinimaxAgent(env, depth=4, epsilon=0, player=1)\n",
    "mini_won = 0\n",
    "mini_lost = 0\n",
    "mini_tie = 0\n",
    "for epi in range(0,1):\n",
    "    finished = -1\n",
    "    env.reset()\n",
    "    while finished == -1:\n",
    "\n",
    "        action = agent.act(env.field)\n",
    "        valid, reward, finished = env.step(action, 2)\n",
    "        env.render_pretty()\n",
    "\n",
    "\n",
    "        if finished != -1:\n",
    "            break\n",
    "\n",
    "        action = opponent.act(env.field)\n",
    "        #action = opponent.act(env.get_state())\n",
    "        valid, reward, finished = env.step(action, 1)\n",
    "        env.render_pretty()\n",
    "\n",
    "\n",
    "        \n",
    "    env.render_pretty()\n",
    "    if finished == 1:\n",
    "        mini_won+=1\n",
    "    elif finished == 2:\n",
    "        mini_lost += 1\n",
    "    else: mini_tie += 1\n",
    "\n",
    "print(f\"Won {mini_won}, Lost: {mini_lost}, Tie: {mini_tie}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = Env()\n",
    "# opponent = MinimaxAgent(env, depth=2, epsilon=0.0, player=2)\n",
    "# traj = transformer_utils.create_trajectory(model, 2, env, opponent, True)\n",
    "# print(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform online learning.\n",
    "ROUNDS = 2000\n",
    "#Top N trajectories\n",
    "N = 500\n",
    "replay_buffer = sorted_offline_trajectories[:N]\n",
    "print(len(replay_buffer))\n",
    "model = model.to(\"cpu\")\n",
    "max_ep_len = max_episode_length\n",
    "print(f\"Maximum Episode Length is {max_ep_len}\")\n",
    "device = \"cpu\"\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"transformers.trainer\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output/\",\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=1, # 120\n",
    "    per_device_train_batch_size=64, # 64\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=0.25,\n",
    "    disable_tqdm=True\n",
    ")\n",
    "\n",
    "TARGET_RETURN = 2\n",
    "env = Env()\n",
    "\n",
    "opponent = opponent = MinimaxAgent(env, depth=2, epsilon=0.0, player=2)\n",
    "\n",
    "oldest_traj = 0\n",
    "EXPLORATORY_MAX = 0.5\n",
    "EXPLORATORY_MIN = 0.1\n",
    "\n",
    "for round in tqdm(range(1, ROUNDS)):\n",
    "    replay_buffer = sorted(replay_buffer, key=lambda x: x[4][0], reverse = True)\n",
    "    print(f\"Round: {round}\")\n",
    "    exploratory = EXPLORATORY_MIN + (EXPLORATORY_MAX - EXPLORATORY_MIN) * (ROUNDS - float(round)) / ROUNDS\n",
    "    print(exploratory)\n",
    "    #perform a trajectory\n",
    "    random.seed(round)\n",
    "    traj1 = transformer_utils.create_trajectory(model, TARGET_RETURN, env, opponent, False, exploratory)\n",
    "    traj2 = transformer_utils.create_trajectory(model, TARGET_RETURN, env, opponent, False, exploratory)\n",
    "    traj3 = transformer_utils.create_trajectory(model, TARGET_RETURN, env, opponent, False, exploratory)\n",
    "    #remove oldest trajectory\n",
    "    #replay_buffer[oldest_traj] = traj1\n",
    "    #replay_buffer[(oldest_traj + 1) % N] = traj2\n",
    "    #replay_buffer[(oldest_traj+ 2) % N] = traj3\n",
    "    #oldest_traj = (oldest_traj + 3) % N\n",
    "    replay_buffer[-1] = traj1\n",
    "    replay_buffer[-2] = traj2\n",
    "    replay_buffer[-3] = traj3\n",
    "\n",
    "\n",
    "    #learn on the new ones..\n",
    "    model = transformer_utils.train_model(model, training_args, replay_buffer, collator)\n",
    "    model = model.to(\"cpu\")\n",
    "    print()\n",
    "\n",
    "\n",
    "#trainer.save_model(\"dt_model/final_random_online\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.cql_agent import CQLAgent\n",
    "from agents.deep_q_agent_double_q import DDQAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your saved model\n",
    "model_path = 'dt_model/final_minimax_online'\n",
    "\n",
    "# Load the model\n",
    "model = transformer_utils.TrainableDT.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: Agent 602 - 392 Opponent. There were 6 Ties\n"
     ]
    }
   ],
   "source": [
    "utils.seed_everything(41)\n",
    "env = Env()\n",
    "# opponent = MinimaxAgent(env, depth=2, epsilon=0.3, player=2)\n",
    "opponent = RandomAgent(env)\n",
    "# opponent = CQLAgent(env, network_type='FCNN', epsilon_max=0.1)\n",
    "# opponent.load_model('./saved_models/CQLAgent_FCNN_RS41.pt')\n",
    "\n",
    "transformer_utils.evaluate_model(model, opponent, 1000, 2, False, agent_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
