{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base libraries\n",
    "from collections import namedtuple, deque\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Callable, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# ML libraries\n",
    "import torch\n",
    "\n",
    "# Local imports\n",
    "from board import ConnectFourField\n",
    "from env import Env\n",
    "from agents.random_agent import RandomAgent\n",
    "from agents.deep_q_agent import DeepQAgent\n",
    "from agents.cql_agent import CQLAgent\n",
    "from agents.minimax_agent import MinimaxAgent\n",
    "import utils\n",
    "\n",
    "# Fix random seed\n",
    "utils.seed_everything()\n",
    "\n",
    "# Import Transformers\n",
    "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''2 Options: \n",
    "        Generate Trajectories now\n",
    "        Load existing Trajectories from a file\n",
    "'''\n",
    "GENERATE_TRAJECTORIES = True #Set if you want to load from file or generate them here\n",
    "\n",
    "# NOTE: Filename style: {agent_name}{input_params}_vs_{opponent_name}{input_params}_num{NUM_TRAJ}.pkl\n",
    "file_to_load_from = 'minimax0_3_vs_minimax0_3_num1000.pkl' #Define file from where to load the trajectories\n",
    "file_to_store_to = 'minimax0_3_vs_minimax0_3_num1000.pkl' #Define file to where we should store the generated trajectories\n",
    "\n",
    "if GENERATE_TRAJECTORIES:\n",
    "    # Generate T_offline (offline Trajectories for the offline initialization of the replay buffer)\n",
    "    # NOTE: alternative would be to initialize with either Random Trajs, or with Traj.s from DQN Agent / CQL Agent\n",
    "    # NOTE: Corresponding paper: https://arxiv.org/pdf/2202.05607.pdf\n",
    "\n",
    "    AGENT = 1\n",
    "    OPPONENT = 2\n",
    "    # Number of created trajectories\n",
    "    NUM_TRAJ = 1000\n",
    "    offline_trajectories = []\n",
    "\n",
    "    #Initialize Environment and Agent\n",
    "    env = Env()\n",
    "    agent = MinimaxAgent(env, epsilon=0.3, whoami=AGENT)\n",
    "    opponent = MinimaxAgent(env, epsilon=0.3, whoami=OPPONENT)\n",
    "\n",
    "    agent_won = 0\n",
    "    agent_lost = 0\n",
    "\n",
    "    for i in range(0, NUM_TRAJ):\n",
    "\n",
    "        # Initialize other variables\n",
    "        finished = -1\n",
    "\n",
    "        #Initialize fields for trajectory\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "\n",
    "        # Make it random who gets to start the game\n",
    "        # Set to true during the episode\n",
    "        agent_start = random.choice([True, False])\n",
    "        # Run one episode of the game\n",
    "        while finished == -1:\n",
    "            # Agent makes a turn\n",
    "            if agent_start:\n",
    "                state = env.get_state()\n",
    "                #action = agent.act(state)\n",
    "                action = agent.act(env.field)\n",
    "                valid, reward, finished = agent.env.step(action, AGENT)\n",
    "                \n",
    "                # Update current Trajectory\n",
    "                #Flatten the state to be a 42-entry 1 dimensional array\n",
    "                states.append(np.ravel(state))\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "\n",
    "                if finished != -1:\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                agent_start = True\n",
    "\n",
    "            # Opponent makes their turn\n",
    "            #action = opponent.act(env.get_state_inverted())\n",
    "            action = opponent.act(env.field)\n",
    "            valid, _, finished = opponent.env.step(action, OPPONENT)\n",
    "\n",
    "            if finished != -1: \n",
    "                break\n",
    "\n",
    "        # The Flag if the Episode is finished is False for n-1 steps and True for the last step obviously..\n",
    "        dones = ([False] * (len(rewards)-1)) + [True]\n",
    "    \n",
    "        assert len(states) == len(actions)\n",
    "        assert len(actions) == len(rewards)\n",
    "        assert len(dones) == len(rewards)\n",
    "        length = len(states)\n",
    "        RTGs = utils.calculate_RTG(rewards)\n",
    "        traj = [length, states, actions, rewards, RTGs, dones]\n",
    "        offline_trajectories.append(traj)\n",
    "\n",
    "        #Keep track of statistics\n",
    "        if finished == 1:\n",
    "            agent_won+=1\n",
    "        elif finished == 2:\n",
    "            agent_lost+=1\n",
    "\n",
    "        if i % int(NUM_TRAJ/10) == 0:\n",
    "            print(f\"Episode: {i}, RTG (start) : {RTGs[0]}, length: {length}, reward end: {rewards[-1]}, done: {dones[-1]}\")\n",
    "            print(f\"Score: Agent {agent_won} - {agent_lost} Opponent\")\n",
    "            #env.render_pretty()\n",
    "        env.reset()\n",
    "        \n",
    "    # Final score\n",
    "    print(f\"Score: Agent {agent_won} - {agent_lost} Opponent. There were {NUM_TRAJ - agent_won - agent_lost} Ties\")\n",
    "\n",
    "    # Sort offline buffer such that the order is descending in RTGs\n",
    "    # [4][0] is the first RTG of the trajectory\n",
    "    sorted_offline_trajectories = sorted(offline_trajectories, key=lambda x: x[4][0], reverse = True)\n",
    "\n",
    "\n",
    "    #TODO: We currently only use one side of the table for generating trajectories, we could generate twice as many by also storing the trajectories from the \"enemys POV\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_TRAJECTORIES:\n",
    "    # HERE WE STORE TRAJECTORIES THAT WE PRODUCES ABOVE, OR ALREADY PRODUCED EARLIER!\n",
    "\n",
    "    #Safety mechanism, to show if this specific file we want to write to doesn't already exist, and we lose our data.\n",
    "    #NOTE: just comment this check out if you want to overwrite a file..\n",
    "    if os.path.exists('trajectories/' + file_to_store_to):\n",
    "        raise FileExistsError(f\"File {file_to_store_to} already exists!\")\n",
    "    \n",
    "    #Create Trajectories folder if in-existent\n",
    "    if not os.path.exists('trajectories'):\n",
    "        os.makedirs('trajectories')\n",
    "    # Store data within specified file\n",
    "    with open(\"trajectories/\" + file_to_store_to, 'wb') as f:\n",
    "        pickle.dump(sorted_offline_trajectories, f)\n",
    "\n",
    "else:\n",
    "    #Check if the given file exists\n",
    "    if os.path.exists('trajectories/' + file_to_load_from):\n",
    "        raise FileExistsError(f\"File {file_to_load_from} doesn't exist!\")\n",
    "    \n",
    "    #Load Trajectories from specified file:\n",
    "    with open('trajectories/' + file_to_load_from, 'rb') as f:\n",
    "        sorted_offline_trajectories = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a dataset containing all the offline collected data\n",
    "# Using this tutorial: https://huggingface.co/blog/train-decision-transformers\n",
    "# We convert the dataset into data ready for the decision transformer\n",
    "FRACTION = 1\n",
    "N = int(FRACTION * NUM_TRAJ)\n",
    "dataset = sorted_offline_trajectories[:N]\n",
    "lengths = [x[0] for x in dataset]\n",
    "max_episode_length = max(lengths)\n",
    "print(f\"Maximum Episode length: {max_episode_length}\")\n",
    "\n",
    "# Constants represent the position of the value within a trajectory-array\n",
    "# trajectory structure in the dataset: traj = [length, states, actions, rewards, RTGs, dones]\n",
    "LENGTH = 0\n",
    "STATES = 1\n",
    "ACTIONS = 2\n",
    "REWARDS = 3\n",
    "RTGs = 4\n",
    "DONES = 5\n",
    "\n",
    "class DecisionTransformerGymDataCollator:\n",
    "    return_tensors: str = \"pt\"\n",
    "    max_len: int = 10 #subsets of the episode we use for training\n",
    "    state_dim: int = 42  # size of state space\n",
    "    act_dim: int = 1  # size of action space\n",
    "    max_ep_len: int = 42 # max episode length in the dataset TODO: is this the correct value?\n",
    "    #scale: float = 1000.0  # normalization of rewards/returns\n",
    "    n_traj: int = N # to store the number of trajectories in the dataset\n",
    "\n",
    "    def __init__(self, dataset) -> None:\n",
    "        self.act_dim = 1\n",
    "        self.state_dim = 42\n",
    "        self.dataset = dataset\n",
    "        self.n_traj = len(dataset)\n",
    "\n",
    "    def _discount_cumsum(self, x, gamma):\n",
    "        discount_cumsum = np.zeros_like(x)\n",
    "        discount_cumsum[-1] = x[-1]\n",
    "        for t in reversed(range(x.shape[0] - 1)):\n",
    "            discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
    "        return discount_cumsum\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch_size = len(features)\n",
    "        # this is a bit of a hack to be able to sample of a non-uniform distribution\n",
    "        batch_inds = np.random.choice(np.arange(self.n_traj), size=batch_size, replace=True)\n",
    "        \n",
    "        # a batch of dataset features\n",
    "        s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
    "        \n",
    "        for ind in batch_inds:\n",
    "            # Select trajectory at given index\n",
    "            feature = self.dataset[int(ind)]\n",
    "            #set si randomly to start somewhere in the sequence (but at least max_len steps before the end..)\n",
    "            length = self.dataset[ind][LENGTH]\n",
    "            if length <= self.max_len:\n",
    "                si = 0\n",
    "            else:\n",
    "                si = random.randint(0, length - self.max_len)\n",
    "\n",
    "            # get sequences from dataset\n",
    "            s.append(np.array(feature[STATES][si : si + self.max_len]).reshape(1, -1, self.state_dim))\n",
    "            a.append(np.array(feature[ACTIONS][si : si + self.max_len]).reshape(1, -1, self.act_dim))\n",
    "            r.append(np.array(feature[REWARDS][si : si + self.max_len]).reshape(1, -1, 1))\n",
    "\n",
    "            d.append(np.array(feature[DONES][si : si + self.max_len]).reshape(1, -1))\n",
    "            timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "            timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
    "            rtg.append(\n",
    "                self._discount_cumsum(np.array(feature[REWARDS][si:]), gamma=1.0)[\n",
    "                    : s[-1].shape[1]   # TODO check the +1 removed here\n",
    "                ].reshape(1, -1, 1)\n",
    "            )\n",
    "            if rtg[-1].shape[1] < s[-1].shape[1]:\n",
    "                print(\"if true\")\n",
    "                rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
    "\n",
    "            # padding and state\n",
    "            tlen = s[-1].shape[1]\n",
    "            s[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, self.state_dim)), s[-1]], axis=1)\n",
    "            a[-1] = np.concatenate(\n",
    "                [np.ones((1, self.max_len - tlen, self.act_dim)) * -10.0, a[-1]],\n",
    "                axis=1,\n",
    "            )\n",
    "            r[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), r[-1]], axis=1)\n",
    "            d[-1] = np.concatenate([np.ones((1, self.max_len - tlen)) * 2, d[-1]], axis=1)\n",
    "            rtg[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), rtg[-1]], axis=1)\n",
    "            timesteps[-1] = np.concatenate([np.zeros((1, self.max_len - tlen)), timesteps[-1]], axis=1)\n",
    "            mask.append(np.concatenate([np.zeros((1, self.max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
    "\n",
    "        # Converting\n",
    "        s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
    "        a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
    "        r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
    "        d = torch.from_numpy(np.concatenate(d, axis=0))\n",
    "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()\n",
    "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()\n",
    "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
    "\n",
    "        # This is how the trajectories are returned for the transformer to learn with them\n",
    "        return {\n",
    "            \"states\": s,\n",
    "            \"actions\": a,\n",
    "            \"rewards\": r,\n",
    "            \"returns_to_go\": rtg,\n",
    "            \"timesteps\": timesteps,\n",
    "            \"attention_mask\": mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is 1:1 from the Huggingface blog, don't know if the loss is correct for our usecase\n",
    "class TrainableDT(DecisionTransformerModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        output = super().forward(**kwargs)\n",
    "        # add the DT loss\n",
    "        action_preds = output[1]\n",
    "        action_targets = kwargs[\"actions\"]\n",
    "        attention_mask = kwargs[\"attention_mask\"]\n",
    "        act_dim = action_preds.shape[2]\n",
    "        action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        \n",
    "        loss = torch.mean((action_preds - action_targets) ** 2)\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def original_forward(self, **kwargs):\n",
    "        return super().forward(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output/\",\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=120,\n",
    "    per_device_train_batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=0.25,\n",
    ")\n",
    "\n",
    "# Initialize and then train the model\n",
    "collator = DecisionTransformerGymDataCollator(dataset)\n",
    "config = DecisionTransformerConfig(state_dim=collator.state_dim, act_dim=collator.act_dim)\n",
    "model = TrainableDT(config)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that gets an action from the model using autoregressive prediction with a window of the previous max_len timesteps.\n",
    "# This function is also directly taken from the Huggingface blog post\n",
    "def get_action(model, states, actions, rewards, returns_to_go, timesteps):\n",
    "    # This implementation does not condition on past rewards\n",
    "\n",
    "    states = states.reshape(1, -1, model.config.state_dim)\n",
    "    actions = actions.reshape(1, -1, model.config.act_dim)\n",
    "    returns_to_go = returns_to_go.reshape(1, -1, 1)\n",
    "    timesteps = timesteps.reshape(1, -1)\n",
    "\n",
    "    states = states[:, -model.config.max_length :]\n",
    "    actions = actions[:, -model.config.max_length :]\n",
    "    returns_to_go = returns_to_go[:, -model.config.max_length :]\n",
    "    timesteps = timesteps[:, -model.config.max_length :]\n",
    "    padding = model.config.max_length - states.shape[1]\n",
    "    # pad all tokens to sequence length\n",
    "    attention_mask = torch.cat([torch.zeros(padding), torch.ones(states.shape[1])])\n",
    "    attention_mask = attention_mask.to(dtype=torch.long).reshape(1, -1)\n",
    "    states = torch.cat([torch.zeros((1, padding, model.config.state_dim)), states], dim=1).float()\n",
    "    actions = torch.cat([torch.zeros((1, padding, model.config.act_dim)), actions], dim=1).float()\n",
    "    returns_to_go = torch.cat([torch.zeros((1, padding, 1)), returns_to_go], dim=1).float()\n",
    "    timesteps = torch.cat([torch.zeros((1, padding), dtype=torch.long), timesteps], dim=1)\n",
    "\n",
    "    state_preds, action_preds, return_preds = model.original_forward(\n",
    "        states=states,\n",
    "        actions=actions,\n",
    "        rewards=rewards,\n",
    "        returns_to_go=returns_to_go,\n",
    "        timesteps=timesteps,\n",
    "        attention_mask=attention_mask,\n",
    "        return_dict=False,\n",
    "    )\n",
    "\n",
    "    return action_preds[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is supposed to evaluate the Decision Transformer against a MinimaxAgent (or RandomAgent) for a fixed amount of Episodes\n",
    "model = model.to(\"cpu\")\n",
    "env = Env()\n",
    "max_ep_len = max_episode_length\n",
    "print(f\"Maximum Episode Length is {max_ep_len}\")\n",
    "device = \"cpu\"\n",
    "TARGET_RETURN = 1\n",
    "\n",
    "opponent = MinimaxAgent(env, epsilon=0.3)\n",
    "\n",
    "for ii in range(0,10):\n",
    "\n",
    "    state_dim = env.field.num_columns * env.field.num_rows\n",
    "    act_dim = 1\n",
    "    # Create the decision transformer model\n",
    "    episode_return, episode_length = 0, 0\n",
    "    env.reset()\n",
    "    state = np.array(env.get_state()).flatten()\n",
    "    target_return = torch.tensor(TARGET_RETURN, device=device, dtype=torch.float32).reshape(1, 1)\n",
    "    states = torch.from_numpy(state).reshape(1, state_dim).to(device=device, dtype=torch.float32)\n",
    "    actions = torch.zeros((0, act_dim), device=device, dtype=torch.float32)\n",
    "    rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
    "\n",
    "    timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)\n",
    "    for t in range(max_ep_len):\n",
    "        actions = torch.cat([actions, torch.zeros((1, act_dim), device=device)], dim=0)\n",
    "        rewards = torch.cat([rewards, torch.zeros(1, device=device)])\n",
    "\n",
    "        action = get_action(\n",
    "            model,\n",
    "            states,\n",
    "            actions,\n",
    "            rewards,\n",
    "            target_return,\n",
    "            timesteps,\n",
    "        )\n",
    "        actions[-1] = action\n",
    "        action = action.detach().cpu().numpy()\n",
    "\n",
    "        #state, reward, done, _ = env.step(action)\n",
    "        valid, reward, finished = env.step(int(action), AGENT)\n",
    "        state = np.array(env.get_state()).flatten()\n",
    "        #env.render_pretty()\n",
    "\n",
    "        if finished != -1:\n",
    "            break\n",
    "        opp_action = opponent.act(env.field)\n",
    "        #print(f\"Opponent Action: {opp_action}\")\n",
    "        valid, _, finished = env.step(int(opp_action), OPPONENT)\n",
    "        state = np.array(env.get_state()).flatten()\n",
    "\n",
    "        cur_state = torch.from_numpy(state).to(device=device).reshape(1, state_dim)\n",
    "        states = torch.cat([states, cur_state], dim=0)\n",
    "        rewards[-1] = reward\n",
    "\n",
    "        pred_return = target_return[0, -1] - reward\n",
    "        target_return = torch.cat([target_return, pred_return.reshape(1, 1)], dim=1)\n",
    "        timesteps = torch.cat([timesteps, torch.ones((1, 1), device=device, dtype=torch.long) * (t + 1)], dim=1)\n",
    "\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        if finished != -1:\n",
    "            break\n",
    "\n",
    "    print(finished)\n",
    "    env.render_pretty()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
